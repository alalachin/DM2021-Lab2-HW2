{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "\n",
    "# with zipfile.ZipFile('dm2021-lab2-hw2.zip', 'r') as zip_ref:\n",
    "#     zip_ref.extractall('./kaggle_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_tweets = pd.read_json(\"./kaggle_data/tweets_DM.json\", lines=True)\n",
    "df_tot = pd.read_csv(\"./kaggle_data/data_identification.csv\")\n",
    "df_emotion = pd.read_csv(\"./kaggle_data/emotion.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observe dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index(['_score', '_index', '_source', '_crawldate', '_type'], dtype='object')\n",
      "<class 'pandas.core.series.Series'> (1867535,)\n",
      "<class 'dict'> dict_keys(['tweet'])\n",
      "<class 'dict'> dict_keys(['hashtags', 'tweet_id', 'text'])\n",
      "{'hashtags': ['Snapchat'], 'tweet_id': '0x376b20', 'text': 'People who post \"add me on #Snapchat\" must be dehydrated. Cuz man.... that\\'s <LH>'}\n",
      "\n",
      "Index(['tweet_id', 'identification'], dtype='object')\n",
      "\n",
      "Index(['tweet_id', 'emotion'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# df_tweets\n",
    "print (type(df_tweets))\n",
    "print (df_tweets.columns)\n",
    "print (type(df_tweets['_source']), df_tweets['_source'].shape)\n",
    "print (type(df_tweets['_source'][0]), df_tweets['_source'][0].keys())\n",
    "print (type(df_tweets['_source'][0]['tweet']), df_tweets['_source'][0]['tweet'].keys())\n",
    "print (df_tweets['_source'][0]['tweet'])\n",
    "print ()\n",
    "\n",
    "# df_tot (train or test)\n",
    "print (df_tot.columns)\n",
    "print ()\n",
    "\n",
    "# df_emotion\n",
    "print (df_emotion.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emotion</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>39867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anticipation</th>\n",
       "      <td>248935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>139101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>63999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>516017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>193437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>48729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trust</th>\n",
       "      <td>205478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id\n",
       "emotion               \n",
       "anger            39867\n",
       "anticipation    248935\n",
       "disgust         139101\n",
       "fear             63999\n",
       "joy             516017\n",
       "sadness         193437\n",
       "surprise         48729\n",
       "trust           205478"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_emotion.groupby('emotion').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are totally 1867535 tweets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'hashtags': ['Snapchat'],\n",
       " 'tweet_id': '0x376b20',\n",
       " 'text': 'People who post \"add me on #Snapchat\" must be dehydrated. Cuz man.... that\\'s <LH>'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"There are totally\", len(df_tweets['_source']), \"tweets\")\n",
    "df_tweets['_source'][0]['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "hashtags = []\n",
    "tweets = []\n",
    "# for i in range(df_tweets.shape[0]):\n",
    "for i in range(100):\n",
    "    twid = df_tweets['_source'][i]['tweet']['tweet_id']\n",
    "    tot = df_tot.loc[df_tot['tweet_id']==twid]\n",
    "    if tot['identification'].to_list()[-1]=='train':\n",
    "#         emo = df_emotion.loc[df_emotion['tweet_id']==twid]['emotion']\n",
    "#         print (emo, (df_tweets['_source'][i]['tweet']['hashtags']))\n",
    "        print (df_tweets['_source'][i]['tweet']['text'])\n",
    "        print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make data_dict and emoji_list\n",
    "\n",
    "data_dict \\[ tweet_id \\] = \\[ text, hashtags, emotion\\ ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "import emoji\n",
    "import pickle\n",
    "\n",
    "reg_tokenizer = RegexpTokenizer(\"\\w+[\\.|\\']\\w+|\\w+|\\#\\w+|\\@\\w+|\\u2764|[\\U0001F600-\\U0001F64F]|[\\U0001F300-\\U0001F5FF]|[\\U0001F680-\\U0001F6FF]|[\\U0001F1E0-\\U0001F1FF]\")\n",
    "\n",
    "\n",
    "def extract_emojis(s):\n",
    "    return ''.join(c for c in s if c in emoji.UNICODE_EMOJI['en'])\n",
    "\n",
    "emoji_set = set()\n",
    "data_dict = {}\n",
    "for i in range(len(df_tweets)):\n",
    "    text = df_tweets['_source'][i]['tweet']['text']\n",
    "    text = text.replace('<LH>', '')\n",
    "    emoji_set = emoji_set.union(set(extract_emojis(text)))\n",
    "    tid = df_tweets['_source'][i]['tweet']['tweet_id']\n",
    "    tags = df_tweets['_source'][i]['tweet']['hashtags']\n",
    "    data_dict[tid] = [text, tags, ''] # [text, hashtags, emo]\n",
    "\n",
    "    # print (text)\n",
    "#     text_list_small.append(text)\n",
    "#     id_list_small.append(tid)\n",
    "#     text = tokenizer.tokenize(text)\n",
    "\n",
    "emoji_set = list(emoji_set)\n",
    "pickle.dump(emoji_set, open('emoji_list.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['People who post \"add me on #Snapchat\" must be dehydrated. Cuz man.... that\\'s ',\n",
       " ['Snapchat'],\n",
       " '']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict['0x376b20']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make data table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for _, row in df_emotion.iterrows():\n",
    "    if row.tweet_id not in data_dict:\n",
    "        print (row.tweet_id)\n",
    "    data_dict[row.tweet_id][-1] = row.emotion\n",
    "    \n",
    "    \n",
    "df_data = pd.DataFrame.from_dict(data_dict, orient='index', columns=['text', 'hashtags', 'emotion'])\n",
    "df_data['ID'] = df_data.index\n",
    "df_data = df_data[['ID', 'text', 'hashtags', 'emotion']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0x376b20</th>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x2d5350</th>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x28b412</th>\n",
       "      <td>0x28b412</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "      <td>[bibleverse]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x1cd5b0</th>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha üòÇüòÇüòÇ</td>\n",
       "      <td>[]</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x2de201</th>\n",
       "      <td>0x2de201</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID                                               text  \\\n",
       "0x376b20  0x376b20  People who post \"add me on #Snapchat\" must be ...   \n",
       "0x2d5350  0x2d5350  @brianklaas As we see, Trump is dangerous to #...   \n",
       "0x28b412  0x28b412  Confident of your obedience, I write to you, k...   \n",
       "0x1cd5b0  0x1cd5b0                    Now ISSA is stalking Tasha üòÇüòÇüòÇ    \n",
       "0x2de201  0x2de201  \"Trust is not the same as faith. A friend is s...   \n",
       "\n",
       "                               hashtags       emotion  \n",
       "0x376b20                     [Snapchat]  anticipation  \n",
       "0x2d5350  [freepress, TrumpLegacy, CNN]       sadness  \n",
       "0x28b412                   [bibleverse]                \n",
       "0x1cd5b0                             []          fear  \n",
       "0x2de201                             []                "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0x376b20</th>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x2d5350</th>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x1cd5b0</th>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha üòÇüòÇüòÇ</td>\n",
       "      <td>[]</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x1d755c</th>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x2c91a8</th>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>Still waiting on those supplies Liscus.</td>\n",
       "      <td>[]</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID                                               text  \\\n",
       "0x376b20  0x376b20  People who post \"add me on #Snapchat\" must be ...   \n",
       "0x2d5350  0x2d5350  @brianklaas As we see, Trump is dangerous to #...   \n",
       "0x1cd5b0  0x1cd5b0                    Now ISSA is stalking Tasha üòÇüòÇüòÇ    \n",
       "0x1d755c  0x1d755c  @RISKshow @TheKevinAllison Thx for the BEST TI...   \n",
       "0x2c91a8  0x2c91a8           Still waiting on those supplies Liscus.    \n",
       "\n",
       "                               hashtags       emotion  \n",
       "0x376b20                     [Snapchat]  anticipation  \n",
       "0x2d5350  [freepress, TrumpLegacy, CNN]       sadness  \n",
       "0x1cd5b0                             []          fear  \n",
       "0x1d755c      [authentic, LaughOutLoud]           joy  \n",
       "0x2c91a8                             []  anticipation  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## make training dataset\n",
    "df_data.loc[df_data['emotion']!=''].to_csv('train_w_hashtag.tsv', sep='\\t', index=False)\n",
    "df_data.loc[df_data['emotion']!=''].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0x28b412</th>\n",
       "      <td>0x28b412</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "      <td>[bibleverse]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x2de201</th>\n",
       "      <td>0x2de201</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x218443</th>\n",
       "      <td>0x218443</td>\n",
       "      <td>When do you have enough ? When are you satisfi...</td>\n",
       "      <td>[materialism, money, possessions]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID                                               text  \\\n",
       "0x28b412  0x28b412  Confident of your obedience, I write to you, k...   \n",
       "0x2de201  0x2de201  \"Trust is not the same as faith. A friend is s...   \n",
       "0x218443  0x218443  When do you have enough ? When are you satisfi...   \n",
       "\n",
       "                                   hashtags emotion  \n",
       "0x28b412                       [bibleverse]          \n",
       "0x2de201                                 []          \n",
       "0x218443  [materialism, money, possessions]          "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## make testing dataset\n",
    "df_data.loc[df_data['emotion']==''].to_csv('test.tsv', sep='\\t', index=False)\n",
    "df_data.loc[df_data['emotion']==''].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenize and vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lihsuan/.local/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:  (1455563, 1000)\n",
      "y_train.shape:  (1455563,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "reg_tokenizer = RegexpTokenizer(\"\\w+[\\.|\\']\\w+|\\w+|\\#\\w+|\\@\\w+|\\u2764|[\\U0001F600-\\U0001F64F]|[\\U0001F300-\\U0001F5FF]|[\\U0001F680-\\U0001F6FF]|[\\U0001F1E0-\\U0001F1FF]\")\n",
    "tf_idf = TfidfVectorizer(max_features=1000, tokenizer=reg_tokenizer.tokenize)\n",
    "tf_idf.fit(df_data['text'])\n",
    "\n",
    "# for a classificaiton problem, you need to provide both training & testing data\n",
    "X = tf_idf.transform(df_data.loc[df_data['emotion']!=\"\"]['text'])\n",
    "y = df_data.loc[df_data['emotion']!=\"\"]['emotion']\n",
    "\n",
    "## take a look at data dimension :)\n",
    "print('X_train.shape: ', X.shape)\n",
    "print('y_train.shape: ', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# decomposition with SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=800, n_iter=200, random_state=42)\n",
    "X = svd.fit_transform(X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# confusion matrix plotting module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, title='Confusion matrix',\n",
    "                          cmap=sns.cubehelix_palette(as_cmap=True)):\n",
    "\n",
    "    classes.sort()\n",
    "    tick_marks = np.arange(len(classes))    \n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           xticklabels = classes,\n",
    "           yticklabels = classes,\n",
    "           title = title,\n",
    "           xlabel = 'True label',\n",
    "           ylabel = 'Predicted label')\n",
    "\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    ylim_top = len(classes) - 0.5\n",
    "    plt.ylim([ylim_top, -.5])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1st model: Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "## build DecisionTree model\n",
    "DT_model = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "## training!\n",
    "DT_model = DT_model.fit(X_train, y_train)\n",
    "\n",
    "## predict!\n",
    "y_train_pred = DT_model.predict(X_train)\n",
    "y_test_pred = DT_model.predict(X_test)\n",
    "\n",
    "acc_train = accuracy_score(y_true=y_train, y_pred=y_train_pred)\n",
    "acc_test = accuracy_score(y_true=y_test, y_pred=y_test_pred)\n",
    "\n",
    "print('training accuracy: {}'.format(round(acc_train, 2)))\n",
    "print('testing accuracy: {}'.format(round(acc_test, 2)))\n",
    "\n",
    "DT_cm = confusion_matrix(y_true=y_test, y_pred=y_test_pred) \n",
    "print(classification_report(y_true=y_test, y_pred=y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tags = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'sadness', 'surprise', 'trust']\n",
    "plot_confusion_matrix(DT_cm, classes=my_tags, title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2nd model: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "RF_model = RandomForestClassifier(max_depth=200)\n",
    "RF_model = RF_model.fit(X_train, y_train)\n",
    "\n",
    "## predict!\n",
    "y_train_pred = RF_model.predict(X_train)\n",
    "y_test_pred = RF_model.predict(X_test)\n",
    "\n",
    "acc_train = accuracy_score(y_true=y_train, y_pred=y_train_pred)\n",
    "acc_test = accuracy_score(y_true=y_test, y_pred=y_test_pred)\n",
    "\n",
    "print('training accuracy: {}'.format(round(acc_train, 2)))\n",
    "print('testing accuracy: {}'.format(round(acc_test, 2)))\n",
    "\n",
    "RF_cm = confusion_matrix(y_true=y_test, y_pred=y_test_pred) \n",
    "print(classification_report(y_true=y_test, y_pred=y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tags = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'sadness', 'surprise', 'trust']\n",
    "plot_confusion_matrix(RF_cm, classes=my_tags, title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3rd model: Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from IPython.display import clear_output\n",
    "from transformers import BertForMaskedLM\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1.0\n",
      "Name: emotion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "tokenizer.add_tokens(emoji_set)\n",
    "    \n",
    "class Dataset(Dataset):\n",
    "    ## read the preprocessed .tsv and  initailize some parameter\n",
    "    def __init__(self, mode, tokenizer):\n",
    "        assert mode in [\"train\", \"test\"]\n",
    "        self.mode = mode\n",
    "        self.df = pd.read_csv(mode + \"_w_hashtag.tsv\", sep=\"\\t\", lineterminator='\\n').fillna(\"\")\n",
    "        self.len = len(self.df)\n",
    "        self.label_map = {'anger':0, 'anticipation':1, 'disgust':2, 'fear':3, 'joy':4, 'sadness':5, 'surprise':6, 'trust':7}\n",
    "        print (self.df.emotion.value_counts() / len(self.df))\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "        tokenizer.add_tokens(emoji_set)\n",
    "        ## will use bert tokenizer\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    \n",
    "    ## return a training / testing data\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == \"test\":\n",
    "            text_id, text_a, text_hashtag = self.df.iloc[idx, :3].values\n",
    "            label_tensor = None\n",
    "        else:\n",
    "            text_id, text_a, text_hashtag, label = self.df.iloc[idx, :].values\n",
    "            \n",
    "            ## convert the label from text to index\n",
    "            ## in order for making tensor\n",
    "            label_id = self.label_map[label]\n",
    "            label_tensor = torch.tensor(label_id)\n",
    "            \n",
    "        ## create BERT token for a tweet\n",
    "        ## add [SEP]\n",
    "        word_pieces = [\"[CLS]\"]\n",
    "        # tokens_a = self.tokenizer.tokenize(text_a)\n",
    "        text_a = text_a + \"\".join(text_hashtag)\n",
    "        tokens_a = self.tokenizer.tokenize(text_a)\n",
    "        word_pieces += tokens_a + [\"[SEP]\"]\n",
    "        len_a = len(word_pieces)\n",
    "        \n",
    "        ## convert tokens sequence to index sequence\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "        tokens_tensor = torch.tensor(ids)\n",
    "        segments_tensor = torch.tensor([0] * len_a, dtype=torch.long)\n",
    "        \n",
    "        return (tokens_tensor, segments_tensor, label_tensor)\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    \n",
    "# ÂàùÂßãÂåñ‰∏ÄÂÄãÂ∞àÈñÄËÆÄÂèñË®ìÁ∑¥Ê®£Êú¨ÁöÑ DatasetÔºå‰ΩøÁî®‰∏≠Êñá BERT Êñ∑Ë©û\n",
    "trainset = Dataset(\"train\", tokenizer=tokenizer)\n",
    "# testset = Dataset(\"test\", tokenizer=tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train set:\n",
    "joy             0.354514\n",
    "anticipation    0.171023\n",
    "trust           0.141167\n",
    "sadness         0.132895\n",
    "disgust         0.095565\n",
    "fear            0.043969\n",
    "surprise        0.033478\n",
    "anger           0.027389"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You don't choose your  üò± but you get to choose your #fancy\\r#FancyVsFear\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1366\n",
    "trainset.df.iloc[1366][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0x1d69d5</th>\n",
       "      <td>0x1d69d5</td>\n",
       "      <td>You don't choose your  üò± but you get to choose...</td>\n",
       "      <td>[fancy, FancyVsFear]</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID                                               text  \\\n",
       "0x1d69d5  0x1d69d5  You don't choose your  üò± but you get to choose...   \n",
       "\n",
       "                      hashtags emotion  \n",
       "0x1d69d5  [fancy, FancyVsFear]    fear  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_emotion.loc[df_emotion['tweet_id']=='0x1d69d5']\n",
    "df_data.loc[df_data['ID']=='0x1d69d5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöConfident of your obedience, I write to you, knowing that you will do even more than I ask. (Philemon 1:21) 3/4 #bibleverse  \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101, 10173, 14810, 32997, 10108, 12787, 34853, 42571, 10421,   117,\n",
      "          151, 26039, 10114, 10855,   117, 72161, 10203, 10855, 11229, 10154,\n",
      "        12818, 10772, 10948,   151, 30316,   119,   113, 17804, 81258,   122,\n",
      "          131, 10259,   114,   124,   120,   125,   108, 24276, 30340,   138,\n",
      "          112, 24276, 30340,   112,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] con ##fi ##dent of your obe ##dien ##ce , i write to you , knowing that you will do even more than i ask . ( phil ##emon 1 : 21 ) 3 / 4 # bible ##verse [ ' bible ##verse ' ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1Ôºö\"Trust is not the same as faith. A friend is someone you trust. Putting faith in anyone is a mistake.\" ~ Christopher Hitchens  \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101,   107, 19367, 10127, 10497, 10103, 11714, 10146, 22033,   119,\n",
      "          143, 17436, 10127, 25839, 10855, 19367,   119, 60622, 22033, 10104,\n",
      "        42946, 10127,   143, 83967,   119,   107,   172, 15550, 14105, 12073,\n",
      "        10107,   138,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] \" trust is not the same as faith . a friend is someone you trust . putting faith in anyone is a mistake . \" ~ christopher hit ##chen ##s [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöWhen do you have enough ? When are you satisfied ? Is you goal really all about money ?  #materialism #money #possessions \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101, 10704, 10154, 10855, 10574, 19351,   136, 10704, 10320, 10855,\n",
      "        90606, 45220, 32245,   136, 10127, 10855, 16110, 25165, 10367, 10935,\n",
      "        15033,   136,   108, 57873, 18771,   108, 15033,   108, 70030,   138,\n",
      "          112, 57873, 18771,   112,   117,   112, 15033,   112,   117,   112,\n",
      "        70030,   112,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] when do you have enough ? when are you sati ##sf ##ied ? is you goal really all about money ? # materiali ##sm # money # possessions [ ' materiali ##sm ' , ' money ' , ' possessions ' ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöGod woke you up, now chase the day #GodsPlan #GodsWork \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101, 13131, 12912, 10499, 10855, 10700,   117, 11628, 24735, 10103,\n",
      "        11111,   108, 28577, 28699,   108, 28577, 36973,   138,   112, 28577,\n",
      "        28699,   112,   117,   112, 28577, 36973,   112,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] god wo ##ke you up , now chase the day # gods ##plan # gods ##work [ ' gods ##plan ' , ' gods ##work ' ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöIn these tough times, who do YOU turn to as your symbol of hope? \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101, 10104, 11269, 65245, 11471,   117, 10488, 10154, 10855, 16582,\n",
      "        10114, 10146, 12787, 24395, 10108, 18763,   136,   138,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] in these tough times , who do you turn to as your symbol of hope ? [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöTurns out you can recognise people by their undies. \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101, 32766, 10871, 10855, 10743, 44909, 12908, 77807, 11227, 10151,\n",
      "        10487, 10138, 11763,   119,   138,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] turns out you can rec ##og ##nise people by their und ##ies . [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöI like how Hayvens mommy, daddy, and the keyboard warriors have to jump into everything. She can‚Äôt handle anything herself. #sheltered \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101,   151, 11531, 12548, 13055, 50498, 61226, 15634,   117, 49741,\n",
      "          117, 10110, 10103, 35659, 30633, 10574, 10114, 25001, 10765, 23225,\n",
      "          119, 10572, 10743,   100,   162, 50483, 32527, 30930,   119,   108,\n",
      "        56434, 10390,   138,   112, 56434, 10390,   112,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] i like how hay ##vens mom ##my , daddy , and the keyboard warriors have to jump into everything . she can [UNK] t handle anything herself . # shelter ##ed [ ' shelter ##ed ' ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöI just love it when every single one of my songs just delete themselves..üò°üòí this is the 3rd times this has happened!  #notamused\n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([   101,    151,  12125,  11157,  10197,  10704,  13667,  11304,  10399,\n",
      "         10108,  11153,  13161,  12125,  26962,  10218,  20319,    119,    119,\n",
      "        106981, 106099,  10372,  10127,  10103,  17070,  11471,  10372,  10438,\n",
      "         35613,    106,    108,  18149,  13203,  10390,    138,    112,  18149,\n",
      "         13203,  10390,    112,    140,    102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] i just love it when every single one of my songs just dele ##te themselves . . üò° üòí this is the 3rd times this has happened ! # nota ##mus ##ed [ ' nota ##mus ##ed ' ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1Ôºö@JulieChen when can we expect a season of #CelebrityBigBrother I think that would be \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101,   137, 18474, 12073, 10704, 10743, 11312, 11460, 84789,   143,\n",
      "        11286, 10108,   108, 44974, 71199, 21722, 38285,   151, 21506, 10203,\n",
      "        11008, 10346,   138,   112, 44974, 71199, 21722, 38285,   112,   140,\n",
      "          102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] @ julie ##chen when can we ex ##pect a season of # celebrity ##big ##bro ##ther i think that would be [ ' celebrity ##big ##bro ##ther ' ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöTbh. Regret hurts more than stepping on a LEGO \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101, 61010, 10243,   119, 47720, 12525, 51416, 10107, 10772, 10948,\n",
      "        21811, 16053, 10125,   143, 52273,   138,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] tb ##h . reg ##ret hurt ##s more than step ##ping on a lego [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöNothing like the delicious taste of coffee and french fries... \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101, 20587, 11531, 10103, 27254, 47838, 44960, 10108, 37181, 10110,\n",
      "        12112, 59135,   119,   119,   119,   138,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] nothing like the deli ##cious taste of coffee and french fries . . . [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöI look at everything  like it will be gone tomorrow \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101,   151, 18480, 10160, 23225, 11531, 10197, 11229, 10346, 23398,\n",
      "        33946,   138,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] i look at everything like it will be gone tomorrow [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöI have plenty of time on my hands, all I do is work - Donald Trump  #almosttheredonald\n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101,   151, 10574, 16360, 87238, 10108, 10573, 10125, 11153, 21809,\n",
      "          117, 10367,   151, 10154, 10127, 11497,   118, 16758, 29104,   108,\n",
      "        16398, 38285, 33954, 15318, 10163,   138,   112, 16398, 38285, 33954,\n",
      "        15318, 10163,   112,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] i have pl ##enty of time on my hands , all i do is work - donald trump # almost ##ther ##edo ##nal ##d [ ' almost ##ther ##edo ##nal ##d ' ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1Ôºö@TheCandaceSmith @lydiabrock I been saying this for 25 years. You are \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101,   137, 10103, 19454, 10250, 15101, 16378, 10243,   137, 54348,\n",
      "        21722, 11732,   151, 10662, 22811, 10372, 10139, 10280, 10868,   119,\n",
      "        10855, 10320,   138,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] @ the ##can ##da ##ces ##mit ##h @ lydia ##bro ##ck i been saying this for 25 years . you are [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1Ôºöforgot to up for the class presentation list but my bff already put my name down üò≠‚úäüèºüëØ Mari gots me. \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([   101,  10139,  31912,  10114,  10700,  10139,  10103,  12728,  33289,\n",
      "         11438,  10502,  11153,  45176,  10481,  18874,  14356,  11153,  11221,\n",
      "         12090, 106580, 106967, 106026, 106002,  17526,  15517,  10107,  10525,\n",
      "           119,    138,    140,    102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] for ##got to up for the class presentation list but my bf ##f already put my name down üò≠ ‚úä üèº üëØ mari got ##s me . [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1Ôºöi'm gonna be house sitting during the new moon \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101,   151,   112,   155, 47016, 10346, 11177, 49436, 10770, 10103,\n",
      "        10246, 15454,   138,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] i ' m gonna be house sitting during the new moon [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöAfter I shoot this back to the Lexus video in la as soon as I get back home I'm going to the @RocNation office ü§∑üèæ‚Äç‚ôÇÔ∏è #partnership  üòÇ\n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([   101,  10515,    151,  37425,  10372,  11677,  10114,  10103,  43737,\n",
      "         10258,  11379,  10104,  10106,  10146,  16211,  10146,    151,  13168,\n",
      "         11677,  11402,    151,    112,    155,  17010,  10114,  10103,    137,\n",
      "         46724,  35408,  12027, 106092, 106150,   1457,    108,  29651, 106541,\n",
      "           138,    112,  29651,    112,    140,    102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] after i shoot this back to the lex ##us video in la as soon as i get back home i ' m going to the @ roc ##nation office ü§∑ üèæ ‚ôÇ # partnership üòÇ [ ' partnership ' ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöWhy is the man from rak su wearing motocross pants in the X factor finals???? \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101, 18469, 10127, 10103, 10564, 10195, 43358, 10192, 51304, 32643,\n",
      "        55912, 14618, 10870, 10104, 10103,   166, 18482, 19037,   136,   136,\n",
      "          136,   136,   138,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] why is the man from rak su wearing moto ##cross pan ##ts in the x factor finals ? ? ? ? [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöAs this new journey begins, I will name her Hope.  You'll have to follow my YouTube channel for details   \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101, 10146, 10372, 10246, 22766, 24318,   117,   151, 11229, 11221,\n",
      "        10483, 18763,   119, 10855,   112, 17361, 10574, 10114, 24024, 11153,\n",
      "        15665, 14145, 10139, 22220,   138,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] as this new journey begins , i will name her hope . you ' ll have to follow my youtube channel for details [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöIÔ∏è just had a random delivery guy check my legs out. He said he could tell I run because I have the legs for it. \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101,   151, 12125, 10407,   143, 29537, 43855, 16153, 24792, 11153,\n",
      "        42492, 10871,   119, 10191, 12338, 10191, 12296, 21166,   151, 12793,\n",
      "        12175,   151, 10574, 10103, 42492, 10139, 10197,   119,   138,   140,\n",
      "          102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] i just had a random delivery guy check my legs out . he said he could tell i run because i have the legs for it . [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1Ôºö@realDonaldTrump It's been on the news all day. Ur just fishing 4 praise. Again. \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101,   137, 11565, 53872, 87263, 12668, 10373, 10197,   112,   161,\n",
      "        10662, 10125, 10103, 11636, 10367, 11111,   119, 10429, 12125, 34969,\n",
      "          125, 56895,   119, 12590,   119,   138,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] @ real ##dona ##ldt ##rum ##p it ' s been on the news all day . ur just fishing 4 praise . again . [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöJust saw the phrase \"luxury fall-out shelter\" in a new story. \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101, 12125, 16289, 10103, 43531,   107, 73497, 13388,   118, 10871,\n",
      "        56434,   107, 10104,   143, 10246, 12159,   119,   138,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] just saw the phrase \" luxury fall - out shelter \" in a new story . [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöI just saw a tumbleweed cross my path for the first time \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101,   151, 12125, 16289,   143, 24875, 11522, 67954, 10163, 13616,\n",
      "        11153, 26584, 10139, 10103, 10403, 10573,   138,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] i just saw a tum ##ble ##wee ##d cross my path for the first time [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1Ôºö@dominicanlil3 Hahahaha i don't feel you #nodoglyf #wishihadadog #lilmelo \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101,   137, 67175, 31097, 10959, 10240, 72219, 62459,   151, 11530,\n",
      "          112,   162, 23333, 10855,   108, 10181, 51661, 10563, 10481,   108,\n",
      "        33020, 14412, 11466, 51661,   108, 43319, 20229, 10132,   138,   112,\n",
      "        10181, 51661, 10563, 10481,   112,   117,   112, 33020, 14412, 11466,\n",
      "        51661,   112,   117,   112, 43319, 20229, 10132,   112,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] @ dominican ##lil ##3 ha ##hah ##aha i don ' t feel you # no ##dog ##ly ##f # wish ##ih ##ada ##dog # lil ##mel ##o [ ' no ##dog ##ly ##f ' , ' wish ##ih ##ada ##dog ' , ' lil ##mel ##o ' ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöFirst day with this kiddos! Going to be a good year!! \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101, 10403, 11111, 10171, 10372, 90395, 10327,   106, 17010, 10114,\n",
      "        10346,   143, 12050, 10817,   106,   106,   138,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] first day with this kidd ##os ! going to be a good year ! ! [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöHaving a routine really does help you get through your week. \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101, 13594,   143, 80084, 25165, 14893, 14743, 10855, 13168, 11284,\n",
      "        12787, 14463,   119,   138,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] having a routine really does help you get through your week . [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1Ôºö@Vikings you guys broke the redskins 29 game sack streak. \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101,   137, 56097, 10855, 67922, 28282, 10103, 65269, 32712, 10397,\n",
      "        11336, 89243, 69217,   119,   138,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] @ vikings you guys broke the reds ##kins 29 game sack streak . [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1Ôºö@GEICO_Service why am I on hold for over 10 mins to request roadside assistance? Mobile app wasn't working. Hung up! \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101,   137, 25463, 12519,   142, 11416, 18469, 10345,   151, 10125,\n",
      "        16473, 10139, 10323, 10148, 12979, 10107, 10114, 31567, 27731, 13181,\n",
      "        28804,   136, 16897, 35821, 54880,   112,   162, 14149,   119, 17522,\n",
      "        10700,   106,   138,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] @ ge ##ico _ service why am i on hold for over 10 min ##s to request roads ##ide assistance ? mobile app wasn ' t working . hung up ! [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöThanks to the awesome team at @LtdMove today for moving us from the Shoebox to the new home and offices.  service\n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101, 47530, 10114, 10103, 37079, 42279, 10688, 10820, 10160,   137,\n",
      "        15198, 58302, 10111, 13980, 10139, 20765, 10763, 10195, 10103, 96104,\n",
      "        32067, 10114, 10103, 10246, 11402, 10110, 27936,   119, 11416,   138,\n",
      "          140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] thanks to the aw ##eso ##me team at @ ltd ##mov ##e today for moving us from the shoe ##box to the new home and offices . service [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöHoly shit confirmed fuck boy. He broke the bro code wowowowow \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101, 20008, 24497, 10123, 26602, 69338, 14140,   119, 10191, 28282,\n",
      "        10103, 26895, 13121, 94608, 27120, 53268,   138,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] holy shi ##t confirmed fuck boy . he broke the bro code wow ##owo ##wow [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1Ôºö@realDonaldTrump Other places have been hit by a Cat 5 hurricane Its not a first You are failing Yes #PuertoRico is island In water \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101,   137, 11565, 53872, 87263, 12668, 10373, 10675, 14937, 10574,\n",
      "        10662, 14105, 10151,   143, 15048,   126, 32561, 10491, 10497,   143,\n",
      "        10403, 10855, 10320, 65350, 31617,   108, 14567, 27031, 10127, 11409,\n",
      "        10104, 11917,   138,   112, 14567, 27031,   112,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] @ real ##dona ##ldt ##rum ##p other places have been hit by a cat 5 hurricane its not a first you are failing yes # puerto ##rico is island in water [ ' puerto ##rico ' ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1Ôºö@realDonaldTrump Did you hear about the fake Fox Seth Rich story? \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101,   137, 11565, 53872, 87263, 12668, 10373, 12266, 10855, 38471,\n",
      "        10935, 10103, 68606, 14530, 39843, 19442, 12159,   136,   138,   140,\n",
      "          102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] @ real ##dona ##ldt ##rum ##p did you hear about the fake fox seth rich story ? [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1Ôºö@realDonaldTrump some people take opioids to mask the pain of having you in the Oval Office \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101,   137, 11565, 53872, 87263, 12668, 10373, 10970, 11227, 11622,\n",
      "        10305, 84762, 13067, 10114, 44054, 10103, 23502, 10108, 13594, 10855,\n",
      "        10104, 10103, 41565, 12027,   138,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] @ real ##dona ##ldt ##rum ##p some people take op ##ioi ##ds to mask the pain of having you in the oval office [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöMy boyfriend and girlfriend both gave me a s/o on insta today üò© \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([   101,  11153,  74159,  10110,  53275,  11422,  15494,  10525,    143,\n",
      "           161,    120,    157,  10125,  52119,  10112,  13980, 106903,    138,\n",
      "           140,    102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] my boyfriend and girlfriend both gave me a s / o on inst ##a today üò© [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöNah seriously how the fk am I gonna pull together $450 for falls when I struggle getting $30 together for an eventü§¶üèº‚Äç‚ôÄÔ∏è \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([   101,  45866,  67679,  12548,  10103,  23151,  10345,    151,  47016,\n",
      "         59289,  13627,    109,  17037,  10139,  17684,  10704,    151,  35379,\n",
      "         27948,    109,  10225,  13627,  10139,  10144,  14225, 106125, 106026,\n",
      "          1456,    138,    140,    102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] nah seriously how the fk am i gonna pull together $ 450 for falls when i struggle getting $ 30 together for an event ü§¶ üèº ‚ôÄ [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1Ôºö@teresamayMP how is it your husbands company haven't paid Corporation Tax for the last five years? \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101,   137, 21321, 80503, 19135, 12548, 10127, 10197, 12787, 19137,\n",
      "        10107, 11062, 24492,   112,   162, 25033, 14439, 22389, 10139, 10103,\n",
      "        11706, 12049, 10868,   136,   138,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] @ teresa ##may ##mp how is it your husband ##s company haven ' t paid corporation tax for the last five years ? [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1Ôºö@VirginTrains Truly shocking service in 1st class this morning, 7.54 Retford-Newcastle. Finally offered coffee at 8.55! \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101,   137, 23157, 13541, 15832, 69434, 36648, 10285, 11416, 10104,\n",
      "        13850, 12728, 10372, 17577,   117,   128,   119, 11829, 44093, 13172,\n",
      "          118, 28025,   119, 18692, 21013, 37181, 10160,   129,   119, 11558,\n",
      "          106,   138,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] @ virgin ##tra ##ins truly shock ##ing service in 1st class this morning , 7 . 54 ret ##ford - newcastle . finally offered coffee at 8 . 55 ! [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöI think I have an obsession. Looking up Damon posters because I have to have one. üòç #VampireDiaries #yes #obsessed \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([   101,    151,  21506,    151,  10574,  10144,  15547,  14357,  15067,\n",
      "           119,  25044,  10700,  46221,  62804,  10107,  12175,    151,  10574,\n",
      "         10114,  10574,  10399,    119, 105946,    108,  34477,  14302,  16362,\n",
      "           108,  31617,    108,  15547,  14357,  16410,    138,    112,  34477,\n",
      "         14302,  16362,    112,    117,    112,  31617,    112,    117,    112,\n",
      "         15547,  14357,  16410,    112,    140,    102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] i think i have an ob ##ses ##sion . looking up damon poster ##s because i have to have one . üòç # vampire ##dia ##ries # yes # ob ##ses ##sed [ ' vampire ##dia ##ries ' , ' yes ' , ' ob ##ses ##sed ' ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöFor better or worst I still will choose you first \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101, 10139, 16197, 10362, 43060,   151, 12440, 11229, 42266, 10855,\n",
      "        10403,   138,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] for better or worst i still will choose you first [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöGuys, the amount of Jinhwi today got me super dizzy. \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101, 67922,   117, 10103, 24205, 10108, 23294, 10243, 15853, 13980,\n",
      "        15517, 10525, 12278, 34026, 12172,   119,   138,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] guys , the amount of jin ##h ##wi today got me super diz ##zy . [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöWell, I did it. I registered to write the @IABC #GCCC #SCMP Exam in #yyc in October. I can't wait! #certification #communications \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101, 11327,   117,   151, 12266, 10197,   119,   151, 31643, 10114,\n",
      "        26039, 10103,   137, 11680, 33160,   108, 70847, 28276,   108, 16427,\n",
      "        19135, 11460, 11064, 10104,   108,   167, 52327, 10104, 11411,   119,\n",
      "          151, 10743,   112,   162, 41550,   106,   108, 56932,   108, 20835,\n",
      "          138,   112, 70847, 28276,   112,   117,   112, 16427, 19135,   112,\n",
      "          117,   112,   167, 52327,   112,   117,   112, 56932,   112,   117,\n",
      "          112, 20835,   112,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] well , i did it . i registered to write the @ ia ##bc # gc ##cc # sc ##mp ex ##am in # y ##yc in october . i can ' t wait ! # certification # communications [ ' gc ##cc ' , ' sc ##mp ' , ' y ##yc ' , ' certification ' , ' communications ' ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöDear Ones, you are not alone, the unseen realm is active beside you. If you struggle, if you need a #home, ask for help.  üåªü•Äüåª\n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([   101,  42958,  28573,    117,  10855,  10320,  10497,  19145,    117,\n",
      "         10103,  15905,  14384,  71698,  10127,  14351,  87862,  10855,    119,\n",
      "         11526,  10855,  35379,    117,  11526,  10855,  15415,    143,    108,\n",
      "         11402,    117,  30316,  10139,  14743,    119, 105944, 106624, 105944,\n",
      "           138,    112,  11402,    112,    140,    102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] dear ones , you are not alone , the uns ##een realm is active beside you . if you struggle , if you need a # home , ask for help . üåª ü•Ä üåª [ ' home ' ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöGreatful for summer but also greatful for gel pens. \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101, 11838, 15836, 10139, 13138, 10502, 10398, 11838, 15836, 10139,\n",
      "        55025, 27435, 10107,   119,   138,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] great ##ful for summer but also great ##ful for gel pen ##s . [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1Ôºö@Thewizz06 @TheHock13 @pdean42 @mudra1975 Showed Ange our GF CREW photo!! Mentioned Supers and GoldenBoys!! #justSayin #Got \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101,   137, 10103, 15853, 56339, 10995, 11325,   137, 10103, 10819,\n",
      "        11732, 42299,   137, 47843, 14985, 62338,   137, 47334, 10281, 49669,\n",
      "        11444, 11301, 27511, 56085, 14008,   149, 10481, 19607, 26188,   106,\n",
      "          106, 24838, 12278, 10107, 10110, 14209, 48082, 10107,   106,   106,\n",
      "          108, 12125, 46659, 10262,   108, 15517,   138,   112, 12125, 46659,\n",
      "        10262,   112,   117,   112, 15517,   112,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] @ the ##wi ##zz ##0 ##6 @ the ##ho ##ck ##13 @ pd ##ean ##42 @ mud ##ra ##19 ##7 ##5 showed ange our g ##f crew photo ! ! mentioned super ##s and golden ##boy ##s ! ! # just ##say ##in # got [ ' just ##say ##in ' , ' got ' ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöIs the grass greener on the other side, or is better the devil you know the right way for now? \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101, 10127, 10103, 34189, 32135, 10131, 10125, 10103, 10675, 12029,\n",
      "          117, 10362, 10127, 16197, 10103, 24004, 10855, 16332, 10103, 12873,\n",
      "        12140, 10139, 11628,   136,   138,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] is the grass greene ##r on the other side , or is better the devil you know the right way for now ? [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöI feel like I'm finally coming into what my purpose is as a human being on this earth. I won't forget this day for a very long time \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101,   151, 23333, 11531,   151,   112,   155, 18692, 19093, 10765,\n",
      "        11523, 11153, 23272, 10127, 10146,   143, 12445, 11352, 10125, 10372,\n",
      "        10204,   119,   151, 11441,   112,   162, 58884, 10372, 11111, 10139,\n",
      "          143, 12495, 11134, 10573,   138,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] i feel like i ' m finally coming into what my purpose is as a human being on this earth . i won ' t forget this day for a very long time [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1Ôºöwhen ur craving sumn and then u get it and it doesn't hit the spot \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101, 10704, 10429, 24590, 32848, 10422, 25049, 10115, 10110, 11120,\n",
      "          163, 13168, 10197, 10110, 10197, 39707,   112,   162, 14105, 10103,\n",
      "        24311,   138,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] when ur cr ##avi ##ng sum ##n and then u get it and it doesn ' t hit the spot [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöI love it when every one is getting along. No fighting, no problem. \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101,   151, 11157, 10197, 10704, 13667, 10399, 10127, 27948, 12396,\n",
      "          119, 10181, 19922,   117, 10181, 15640,   119,   138,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] i love it when every one is getting along . no fighting , no problem . [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1Ôºö@heidiklum @TimGunn @georginachapman Which of all of you knew about Weinstein sexually harassing the Project Runway models?  \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101,   137, 60544, 43426, 10150,   137, 12268, 20286, 10115,   137,\n",
      "        79343, 28906, 17029, 10629, 10359, 10108, 10367, 10108, 10855, 36963,\n",
      "        10935, 78654, 15893, 18333, 10563, 35548, 57073, 10103, 12078, 65901,\n",
      "        20070,   136,   138,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] @ heidi ##klu ##m @ tim ##gun ##n @ georgi ##nach ##ap ##man which of all of you knew about wein ##stein sexual ##ly hara ##ssing the project runway models ? [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöHumans are still Humans it dont matter what color ur skin is or where u come from #loveislove \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101, 24116, 10320, 12440, 24116, 10197, 11930, 22917, 11523, 13749,\n",
      "        10429, 28106, 10127, 10362, 10982,   163, 10695, 10195,   108, 11157,\n",
      "        10310, 46154,   138,   112, 11157, 10310, 46154,   112,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] humans are still humans it dont matter what color ur skin is or where u come from # love ##is ##love [ ' love ##is ##love ' ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöWhen your husband forgots  to turn his alarm clock off and its a bank holiday. That. \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101, 10704, 12787, 19137, 10139, 31912, 10107, 10114, 16582, 10235,\n",
      "        68859, 33844, 11856, 10110, 10491,   143, 12638, 28781,   119, 10203,\n",
      "          119,   138,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] when your husband for ##got ##s to turn his alarm clock off and its a bank holiday . that . [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1Ôºö@HewzyLFC Them silly fuck wits r clueless!!! The way Klopp is set up!!! He's happy 4 SURE... \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101,   137, 10191, 10650, 12172, 31226, 10261, 11359, 25708, 10563,\n",
      "        69338, 31319, 10107,   160, 33631, 58174, 13934,   106,   106,   106,\n",
      "        10103, 12140, 43298, 85033, 10127, 10486, 10700,   106,   106,   106,\n",
      "        10191,   112,   161, 19308,   125, 26210,   119,   119,   119,   138,\n",
      "          140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] @ he ##w ##zy ##lf ##c them sil ##ly fuck wit ##s r cl ##uele ##ss ! ! ! the way kl ##opp is set up ! ! ! he ' s happy 4 sure . . . [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöDQs Pumpkin Pie blizzard is awful! It tastes like squash baby food ü§¢ü§¢ #nothanks #babyfood  #trashit\n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([   101,  80589,  10107,  63731,  12497,  17322,  91147,  10127,  37079,\n",
      "         15836,    106,  10197,  44960,  10107,  11531,  26469,  44807,  10243,\n",
      "         15719,  15225, 106537, 106537,    108,  10497,  12134,  12009,    108,\n",
      "         15719,  22046,  13227,    108,  13083,  36381,    138,    112,  10497,\n",
      "         12134,  12009,    112,    117,    112,  15719,  22046,  13227,    112,\n",
      "           117,    112,  13083,  36381,    112,    140,    102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] dq ##s pump ##kin pie blizzard is aw ##ful ! it taste ##s like sq ##uas ##h baby food ü§¢ ü§¢ # not ##han ##ks # baby ##fo ##od # tras ##hit [ ' not ##han ##ks ' , ' baby ##fo ##od ' , ' tras ##hit ' ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöFederal Building ü•óü•ó 11000 Wilshire Blvd Los Angeles,California 90024 Serving 11-2pm!! #westwood \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([   101,  12501,  12262, 106978, 106978,  23799,  10995,  28623,  18632,\n",
      "         19588,  47954,  10175,  12258,    117,  11653,  12806,  48626,  20182,\n",
      "         10170,    118,    123,  46445,    106,    106,    108,  57859,    138,\n",
      "           112,  57859,    112,    140,    102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] federal building ü•ó ü•ó 1100 ##0 wil ##shire bl ##vd los angeles , california 900 ##24 serving 11 - 2 ##pm ! ! # westwood [ ' westwood ' ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1Ôºö@CNN She will say anything to defend her 8 million dollar mortgage! \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101,   137, 30047, 10572, 11229, 16497, 32527, 10114, 43461, 10483,\n",
      "          129, 12427, 18494, 12298, 80635,   106,   138,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] @ cnn she will say anything to defend her 8 million dollar mort ##gage ! [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1Ôºöjust watched #BabyDriverMovie at the cinema, never seen music fused with life like inside my head before &lt;3 \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101, 12125, 84447,   108, 15719, 75034, 28558, 16136, 10111, 10160,\n",
      "        10103, 13665,   117, 13362, 15299, 10838, 10961, 16410, 10171, 10287,\n",
      "        11531, 17490, 11153, 12349, 11364,   111, 23560,   132,   124,   138,\n",
      "          112, 15719, 75034, 28558, 16136, 10111,   112,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] just watched # baby ##drive ##rm ##ovi ##e at the cinema , never seen music fu ##sed with life like inside my head before & lt ; 3 [ ' baby ##drive ##rm ##ovi ##e ' ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöSo much talk about #Scaramucci , Scaramucci, but no one has asked will he do the fahndango.  \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101, 10297, 12977, 20220, 10935,   108, 92672, 54924, 27200,   117,\n",
      "        92672, 54924, 27200,   117, 10502, 10181, 10399, 10438, 21675, 11229,\n",
      "        10191, 10154, 10103, 11403, 17278, 39243, 10132,   119,   138,   112,\n",
      "        92672, 54924, 27200,   112,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] so much talk about # scar ##amu ##cci , scar ##amu ##cci , but no one has asked will he do the fa ##hn ##dang ##o . [ ' scar ##amu ##cci ' ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöI'm playing against Deshaun in fantasy. I hope he does well tonight but the RBs get all the TDs. \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101,   151,   112,   155, 14734, 11423, 10143, 45641, 10115, 10104,\n",
      "        18532,   119,   151, 18763, 10191, 14893, 11327, 31200, 10502, 10103,\n",
      "        58058, 10107, 13168, 10367, 10103, 56090, 10107,   119,   138,   140,\n",
      "          102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] i ' m playing against des ##hau ##n in fantasy . i hope he does well tonight but the rb ##s get all the td ##s . [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöAccidentally deleted the video I was going to upload to my channel üò´üò°üò§ #dumbass \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([   101,  93838,  26962,  11894,  10103,  11379,    151,  10140,  17010,\n",
      "         10114,  10700,  55343,  10114,  11153,  14145, 106303, 106981, 105995,\n",
      "           108,  26728,  15566,  10107,    138,    112,  26728,  15566,  10107,\n",
      "           112,    140,    102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] accidentally dele ##ted the video i was going to up ##load to my channel üò´ üò° üò§ # dum ##bas ##s [ ' dum ##bas ##s ' ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöStarting high school in 3 days I'm so not ready Im going to get so lost üòÇ pray for me guysüò´üôèüèª #highschool  \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([   101,  19608,  11053,  10666,  10104,    124,  12889,    151,    112,\n",
      "           155,  10297,  10497,  27080,  10205,  17010,  10114,  13168,  10297,\n",
      "         12754, 106541,  43898,  10158,  10139,  10525,  67922, 106303, 106700,\n",
      "        106752,    108,  11053,  41999,    138,    112,  11053,  41999,    112,\n",
      "           140,    102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] starting high school in 3 days i ' m so not ready im going to get so lost üòÇ pra ##y for me guys üò´ üôè üèª # high ##school [ ' high ##school ' ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöSomeone anyone hit my  DM ‚Äôs...  I‚Äôm bored #bored #dm  \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101, 25839, 42946, 14105, 11153, 41083,   100,   161,   119,   119,\n",
      "          119,   151,   100,   155, 57550, 10163,   108, 57550, 10163,   108,\n",
      "        41083,   138,   112, 57550, 10163,   112,   117,   112, 41083,   112,\n",
      "          140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] someone anyone hit my dm [UNK] s . . . i [UNK] m bore ##d # bore ##d # dm [ ' bore ##d ' , ' dm ' ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1Ôºö#cryptonerds 60 push-ups and 60 squats every 24 hours. Then get back to #blockchain \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101,   108, 29917, 63427, 15222, 10107, 10780, 43406,   118, 61729,\n",
      "        10110, 10780, 26469, 38329, 10107, 13667, 10227, 18030,   119, 11120,\n",
      "        13168, 11677, 10114,   108, 18612, 14046, 10262,   138,   112, 29917,\n",
      "        63427, 15222, 10107,   112,   117,   112, 18612, 14046, 10262,   112,\n",
      "          140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] # cry ##pton ##erd ##s 60 push - ups and 60 sq ##uat ##s every 24 hours . then get back to # block ##cha ##in [ ' cry ##pton ##erd ##s ' , ' block ##cha ##in ' ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöHar¬£ee Reitzel, you have been my saving grace thru the storm. ‚ô°‚ô° \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101, 10347, 96659, 12168, 13380, 50717, 10159,   117, 10855, 10574,\n",
      "        10662, 11153, 53938, 14586, 29263, 11859, 10103, 18122,   119,  1464,\n",
      "        97334,   138,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] har ##¬£ ##ee rei ##tze ##l , you have been my saving grace th ##ru the storm . ‚ô° ##‚ô° [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1Ôºö@srlcare #cheap gimmicks by SRL that phine was not reachable try something new. \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101,   137, 16471, 79333, 10247,   108, 78026, 21464, 93046, 28144,\n",
      "        10151, 16471, 10159, 10203, 18369, 10220, 10140, 10497, 22074, 13356,\n",
      "        26333, 19501, 10246,   119,   138,   112, 78026,   112,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] @ sr ##lca ##re # cheap gi ##mmi ##cks by sr ##l that phi ##ne was not reach ##able try something new . [ ' cheap ' ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1Ôºö@Hind_Modaimegh My cat says that to me every morning \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101,   137, 67602,   142, 31591, 20204, 14547, 11153, 15048, 20782,\n",
      "        10203, 10114, 10525, 13667, 17577,   138,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] @ hind _ moda ##ime ##gh my cat says that to me every morning [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöWaiting for my mac to update üòæ LOOONNGGGG \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([   101,  33542,  10139,  11153,  13501,  10114,  24531, 106344,  10387,\n",
      "         15845,  10422,  20738,  10251,    138,    140,    102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] waiting for my mac to update üòæ lo ##oon ##ng ##gg ##g [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1Ôºö@timesofindia @MamataOfficial Look who's talking abt culture n Heritage !! \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101,   137, 11471, 85106, 38575,   137, 22802, 38420, 70381, 40638,\n",
      "        18480, 10488,   112,   161, 39643, 47678, 11828,   156, 17328,   106,\n",
      "          106,   138,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] @ times ##ofi ##ndia @ mama ##tao ##ffi ##cial look who ' s talking abt culture n heritage ! ! [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöPraying more for @vancepitman, pastors and Believers in Las Vegas area during aftermath of this nations largest mass shooting. \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101, 43898, 38101, 10772, 10139,   137, 80994, 32750, 10629,   117,\n",
      "        25991, 10107, 10110, 22142, 11082, 10104, 10265, 21766, 10793, 10770,\n",
      "        68419, 10108, 10372, 14268, 15474, 17141, 29268,   119,   138,   140,\n",
      "          102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] pra ##ying more for @ vance ##pit ##man , pastor ##s and believe ##rs in las vegas area during aftermath of this nations largest mass shooting . [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöIt's October and now I can watch Ghost adventures on repeat. \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101, 10197,   112,   161, 11411, 10110, 11628,   151, 10743, 20367,\n",
      "        23319, 25687, 10125, 81120,   119,   138,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] it ' s october and now i can watch ghost adventures on repeat . [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöThe things my friends do for me just leave me like WOW! #FeelingSpecial \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101, 10103, 17994, 11153, 16119, 10154, 10139, 10525, 12125, 19524,\n",
      "        10525, 11531, 94608,   106,   108, 56092, 56478, 17333,   138,   112,\n",
      "        56092, 56478, 17333,   112,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] the things my friends do for me just leave me like wow ! # feelings ##pec ##ial [ ' feelings ##pec ##ial ' ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöJust got invited to a kids birthday party by the person who manages our paper goods. She told me to bring my children. Im \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101, 12125, 15517, 31867, 10114,   143, 23800, 31551, 11727, 10151,\n",
      "        10103, 14150, 10488, 71695, 14008, 15994, 32714,   119, 10572, 20917,\n",
      "        10525, 10114, 22401, 11153, 12171,   119, 10205,   138,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] just got invited to a kids birthday party by the person who manages our paper goods . she told me to bring my children . im [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöYou built a fire for the both of us to feel but now, it's being put out by my tears. \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101, 10855, 12321,   143, 12319, 10139, 10103, 11422, 10108, 10763,\n",
      "        10114, 23333, 10502, 11628,   117, 10197,   112,   161, 11352, 14356,\n",
      "        10871, 10151, 11153, 39214,   119,   138,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] you built a fire for the both of us to feel but now , it ' s being put out by my tears . [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöOK, but seriously, we're not getting just 1 card reveal today, right? üò∞ #FrozenThrone \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([   101,  13563,    117,  10502,  67679,    117,  11312,    112,  11449,\n",
      "         10497,  27948,  12125,    122,  18579,  53468,  13980,    117,  12873,\n",
      "           136, 106406,    108,  61195,  11434,  39789,    138,    112,  61195,\n",
      "         11434,  39789,    112,    140,    102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] ok , but seriously , we ' re not getting just 1 card reveal today , right ? üò∞ # frozen ##th ##rone [ ' frozen ##th ##rone ' ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1Ôºö\"For by grace  you are saved through faith; and that not of yourselves: it is the gift of God.\" -Ephesians 2:8   \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101,   107, 10139, 10151, 14586, 10855, 10320, 46422, 11284, 22033,\n",
      "          132, 10110, 10203, 10497, 10108, 93252, 10624, 13193,   131, 10197,\n",
      "        10127, 10103, 18644, 10108, 13131,   119,   107,   118, 13734, 18969,\n",
      "        38300,   123,   131,   129,   138,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] \" for by grace you are saved through faith ; and that not of yours ##el ##ves : it is the gift of god . \" - ep ##hes ##ians 2 : 8 [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöPlanning A Disney Vacay With My Baby üíûüòç \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([   101,  22648,    143,  15258,  85599,  10158,  10171,  11153,  15719,\n",
      "        106292, 105946,    138,    140,    102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] planning a disney vaca ##y with my baby üíû üòç [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1Ôºö@gizmostripeycat @CanD_MK @SoCalBarb @ckkoch3 @CharlesMBlow Oh my...you don't understand what a \"play on words\" is either. \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101,   137, 21464, 28958, 20109, 56523, 13658, 18046,   137, 19944,\n",
      "          142, 20478,   137, 17727, 10476, 13303, 10417,   137, 81895, 10457,\n",
      "        10277, 10959,   137, 11049, 42541, 22057, 17003, 11153,   119,   119,\n",
      "          119, 10855, 11530,   112,   162, 43996, 11523,   143,   107, 11923,\n",
      "        10125, 18281,   107, 10127, 16342,   119,   138,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] @ gi ##zm ##ost ##rip ##ey ##cat @ cand _ mk @ soc ##al ##bar ##b @ ck ##ko ##ch ##3 @ charles ##mb ##low oh my . . . you don ' t understand what a \" play on words \" is either . [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöI have a closet w clothes,shoes, purses, 2 kids who‚ù§Ô∏ème, excellent health etc-money won't change my life-prayin 4 honest Loveüíòüôèüòä #amor \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([   101,    151,  10574,    143,  15130,  10123,    165,  67260,    117,\n",
      "         57111,    117,  27327,  14357,    117,    123,  23800,  10488, 106108,\n",
      "         10525,    117,  42700,  13347,  12575,    118,  15033,  11441,    112,\n",
      "           162,  13780,  11153,  10287,    118,  43898,  61847,    125,  11817,\n",
      "         14324,  11157, 106393, 106700, 106683,    108,  14199,    138,    112,\n",
      "         14199,    112,    140,    102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] i have a close ##t w clothes , shoes , pur ##ses , 2 kids who ‚ù§ me , excellent health etc - money won ' t change my life - pra ##yin 4 hon ##est love üíò üôè üòä # amor [ ' amor ' ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1Ôºö@realDonaldTrump Please just STFU!  You are the biggest fucking moron of all times!   \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101,   137, 11565, 53872, 87263, 12668, 10373, 38881, 12125, 10836,\n",
      "        17848,   106, 10855, 10320, 10103, 31575, 69338, 10285, 50371, 10115,\n",
      "        10108, 10367, 11471,   106,   138,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] @ real ##dona ##ldt ##rum ##p please just st ##fu ! you are the biggest fuck ##ing moro ##n of all times ! [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöAs if some random bloke wi his family just paid for my order at maccies drive thru  üôåüôå\n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([   101,  10146,  11526,  10970,  29537,  39052,  10111,  19078,  10235,\n",
      "         11214,  12125,  25033,  10139,  11153,  11970,  10160,  13501,  22208,\n",
      "         15834,  29263,  11859, 106199, 106199,    138,    140,    102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] as if some random blok ##e wi his family just paid for my order at mac ##cies drive th ##ru üôå üôå [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1Ôºö@realDonaldTrump Once more applauding bigotry and racism. A clear pattern of where you stand. #notamerican  #notmypresident\n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101,   137, 11565, 53872, 87263, 12668, 10373, 12983, 10772, 35821,\n",
      "        36500, 14043, 12062, 11218, 11160, 10110, 86422, 14081,   119,   143,\n",
      "        20674, 32228, 10108, 10982, 10855, 12675,   119,   108, 18149, 91782,\n",
      "        19454,   108, 10497, 15634, 67187,   138,   112, 18149, 91782, 19454,\n",
      "          112,   117,   112, 10497, 15634, 67187,   112,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] @ real ##dona ##ldt ##rum ##p once more app ##lau ##ding big ##ot ##ry and rac ##ism . a clear pattern of where you stand . # nota ##meri ##can # not ##my ##president [ ' nota ##meri ##can ' , ' not ##my ##president ' ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1Ôºö@CumBubbleJesus Oddly enough I couldn't remember my login for them this morning then it came to me when I saw this. \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101,   137, 15175, 12217, 82178, 15661, 10258, 39032, 10563, 19351,\n",
      "          151, 83643,   112,   162, 34615, 11153, 32314, 10262, 10139, 11359,\n",
      "        10372, 17577, 11120, 10197, 13430, 10114, 10525, 10704,   151, 16289,\n",
      "        10372,   119,   138,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] @ cum ##bu ##bble ##jes ##us odd ##ly enough i couldn ' t remember my log ##in for them this morning then it came to me when i saw this . [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1Ôºö8 years ago today, I became friends with people that gave me the most incredible experiences and memories, forever grateful! \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101,   129, 10868, 30526, 13980,   117,   151, 11365, 16119, 10171,\n",
      "        11227, 10203, 15494, 10525, 10103, 10889, 81981, 32419, 10110, 34749,\n",
      "          117, 24886, 29280, 10218, 15836,   106,   138,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] 8 years ago today , i became friends with people that gave me the most incredible experiences and memories , forever gra ##te ##ful ! [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöIs Rick Perry implying that people don‚Äôt get beat with the lights on?  #resist\n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101, 10127, 20454, 20122, 73296, 83886, 10203, 11227, 11530,   100,\n",
      "          162, 13168, 18477, 10171, 10103, 30440, 10125,   136,   108, 20776,\n",
      "        11633,   138,   112, 20776, 11633,   112,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] is rick perry imp ##lying that people don [UNK] t get beat with the lights on ? # res ##ist [ ' res ##ist ' ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöPerfect end to a fab weekend. Reading a book in a bubble bath.  \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101, 23021, 11421, 10114,   143, 78491, 28326,   119, 10504,   143,\n",
      "        11768, 10104,   143, 89649, 37556,   119,   138,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] perfect end to a fab weekend . reading a book in a bubble bath . [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöAnd even if I was afraid, even if I was running away from her love is a non-degradable atom.  #bepatient #dontbeafraid\n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101, 10110, 12818, 11526,   151, 10140, 84485,   117, 12818, 11526,\n",
      "          151, 10140, 16484, 13795, 10195, 10483, 11157, 10127,   143, 10466,\n",
      "          118, 88831, 18179, 11522, 27155,   119,   108, 10346, 41776, 11604,\n",
      "          108, 11930, 83696, 28695, 11296,   138,   112, 10346, 41776, 11604,\n",
      "          112,   117,   112, 11930, 83696, 28695, 11296,   112,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] and even if i was afraid , even if i was running away from her love is a non - deg ##rada ##ble atom . # be ##pati ##ent # dont ##bea ##fra ##id [ ' be ##pati ##ent ' , ' dont ##bea ##fra ##id ' ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1Ôºöwoke up to the best text in the worldüíñ \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([   101,  12912,  10499,  10700,  10114,  10103,  11146,  14059,  10104,\n",
      "         10103,  10228, 106022,    138,    140,    102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] wo ##ke up to the best text in the world üíñ [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöNo matter what happens in life , I know I can always count on my wonderful family, my loving husband & my caring friends üíï \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([   101,  10181,  22917,  11523,  76959,  10104,  10287,    117,    151,\n",
      "         16332,    151,  10743,  17503,  22949,  10125,  11153,  49108,  11214,\n",
      "           117,  11153,  51939,  19137,    111,  11153,  12485,  10285,  16119,\n",
      "        105951,    138,    140,    102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] no matter what happens in life , i know i can always count on my wonderful family , my loving husband & my car ##ing friends üíï [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1Ôºö@SkySportsWWE @SummerSlam Hope I win never been able to abroad and always dreamed of going to a big wrestling event \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101,   137, 13724, 41627, 10107, 10650, 12351,   137, 58096, 23621,\n",
      "        18763,   151, 13997, 13362, 10662, 16368, 10114, 42570, 10110, 17503,\n",
      "        16121, 10390, 10108, 17010, 10114,   143, 12062, 16851, 14225,   138,\n",
      "          140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] @ sky ##sport ##s ##w ##we @ summers ##lam hope i win never been able to abroad and always dream ##ed of going to a big wrestling event [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöHow would Gordon Ramsay rate and react to the jaulo I just had üòØ \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([   101,  12548,  11008,  14988,  68803,  17593,  10110,  78949,  15106,\n",
      "         10114,  10103,  30529,  10687,    151,  12125,  10407, 106420,    138,\n",
      "           140,    102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] how would gordon ramsay rate and rea ##ct to the jau ##lo i just had üòØ [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöCozy is a vibe go listen to it, I'm really proud of you Jacob @jacobsartorius  ‚ù§\n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([   101,  25889,  10158,  10127,    143,  10934,  11044,  11335,  32738,\n",
      "         10114,  10197,    117,    151,    112,    155,  25165,  59386,  10108,\n",
      "         10855,  16864,    137,  40732,  15358,  14503,  10258, 106108,    138,\n",
      "           140,    102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] coz ##y is a vi ##be go listen to it , i ' m really proud of you jacob @ jacobs ##art ##ori ##us ‚ù§ [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1Ôºö#FiEurope2017 is about to open and we can't wait. Come say HI at stand 08.0R65 \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101,   108, 29031, 21012, 11599, 22734, 33012, 10127, 10935, 10114,\n",
      "        11481, 10110, 11312, 10743,   112,   162, 41550,   119, 10695, 16497,\n",
      "        11463, 10160, 12675, 10862,   119,   121, 10131, 73311,   138,   112,\n",
      "        29031, 21012, 11599, 22734, 33012,   112,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] # fie ##uro ##pe ##20 ##17 is about to open and we can ' t wait . come say hi at stand 08 . 0 ##r ##65 [ ' fie ##uro ##pe ##20 ##17 ' ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöSita :  Who are You?  Desperate Ravan : I am DespaSito    Pj\n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101, 18459,   131, 10488, 10320, 10855,   136, 71081, 11742, 12459,\n",
      "          131,   151, 10345, 10143, 85378, 10366, 82579,   138,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] sita : who are you ? desperate ra ##van : i am des ##pasi ##to pj [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöIn other news, football is almost back. And I'm alive. \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101, 10104, 10675, 11636,   117, 11420, 10127, 16398, 11677,   119,\n",
      "        10110,   151,   112,   155, 24758,   119,   138,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] in other news , football is almost back . and i ' m alive . [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöSo I have to assist you on how to exit the building? Do you need me to wipe your ass too?!? üíÅüèª #CustomerExperience \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([   101,  10297,    151,  10574,  10114,  37514,  10855,  10125,  12548,\n",
      "         10114,  24794,  10103,  12262,    136,  10154,  10855,  15415,  10525,\n",
      "         10114,  19078,  11599,  12787,  13967,  14666,    136,    106,    136,\n",
      "        105921, 106752,    108,  55613,  30754,  66049,  23218,    138,    112,\n",
      "         55613,  30754,  66049,  23218,    112,    140,    102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] so i have to assist you on how to exit the building ? do you need me to wi ##pe your ass too ? ! ? üíÅ üèª # customer ##ex ##peri ##ence [ ' customer ##ex ##peri ##ence ' ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöYAYYYY!!!! Congrats to my #LADodgers #CodyBellinger ROOKIE OF THE YEAR!!!!!!!!!!!!!!!!!!!!!!!! #ThisTeamLA \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101, 10593, 32919, 32919,   106,   106,   106,   106, 11638, 29719,\n",
      "        10114, 11153,   108, 15841, 27690, 11082,   108, 58086, 95000, 20285,\n",
      "        35955, 10108, 10103, 10817,   106,   106,   106,   106,   106,   106,\n",
      "          106,   106,   106,   106,   106,   106,   106,   106,   106,   106,\n",
      "          106,   106,   106,   106,   106,   106,   106,   106,   108, 10372,\n",
      "        50340, 10335,   138,   112, 15841, 27690, 11082,   112,   117,   112,\n",
      "        58086, 95000, 20285,   112,   117,   112, 10372, 50340, 10335,   112,\n",
      "          140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] ya ##yy ##yy ! ! ! ! cong ##rats to my # lado ##dge ##rs # cody ##bell ##inger rookie of the year ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! # this ##team ##la [ ' lado ##dge ##rs ' , ' cody ##bell ##inger ' , ' this ##team ##la ' ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1Ôºö@jessesingal dang it. Before the pic loaded I was hoping for baby geese. Instead got booze & some dude  üòÜ\n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([   101,    137,  27044,  17557,  10476,  22548,  10197,    119,  11364,\n",
      "         10103,  29756,  70370,    151,  10140,  13365,  10285,  10139,  15719,\n",
      "         59801,  10352,    119,  16209,  15517,  86696,  10732,    111,  10970,\n",
      "         60537,  10111, 106270,    138,    140,    102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] @ jesse ##sing ##al dang it . before the pic loaded i was hop ##ing for baby gee ##se . instead got boo ##ze & some dud ##e üòÜ [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöUnstable and not competent to lead were the obvious flaws of the man DURING THE PRIMARIES! \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101, 15905, 33832, 10110, 10497, 32756, 10368, 10114, 13868, 10342,\n",
      "        10103, 84856, 19341, 29274, 10107, 10108, 10103, 10564, 10770, 10103,\n",
      "        74375, 11763,   106,   138,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] uns ##table and not compete ##nt to lead were the obvious fl ##aw ##s of the man during the primar ##ies ! [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1ÔºöSaturday night with Christian and Lucy in Wiltshire was such a vibe!!  Thanks for having us, have a lovely honeymoon  #feverforever\n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101, 24989, 12074, 10171, 11965, 10110, 26580, 10104, 76438, 10140,\n",
      "        11165,   143, 10934, 11044,   106,   106, 47530, 10139, 13594, 10763,\n",
      "          117, 10574,   143, 83944, 38242, 11246, 10273,   108, 40179, 18527,\n",
      "        44629,   138,   112, 40179, 18527, 44629,   112,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] saturday night with christian and lucy in wiltshire was such a vi ##be ! ! thanks for having us , have a lovely honey ##mo ##on # fever ##for ##ever [ ' fever ##for ##ever ' ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1Ôºö@PAOnTheMic It‚Äôs working out House of Pain in full effect! Gotta hit those two though! \n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101,   137, 67995, 58035, 33819, 10197,   100,   161, 14149, 10871,\n",
      "        11177, 10108, 23502, 10104, 12851, 17400,   106, 93954, 14105, 12671,\n",
      "        10536, 14325,   106,   138,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] @ pao ##nthe ##mic it [UNK] s working out house of pain in full effect ! gotta hit those two though ! [ ] [SEP]\n",
      "        \n",
      "[ÂéüÂßãÊñáÊú¨]\n",
      "        Âè•Â≠ê 1Ôºö@maddow well the far-right Patriot that assassinates Bergdahl use the commander and chief instructions as a defense?  #believeme\n",
      "        ÂàÜÈ°û  Ôºö\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
      "        tokens_tensor  Ôºötensor([  101,   137, 24603, 38027, 11327, 10103, 12218,   118, 12873, 75306,\n",
      "        10203, 52702, 10165, 12107, 94967, 11868, 10103, 18099, 10110, 13986,\n",
      "        58179, 10146,   143, 15610,   136,   108, 22142, 10688,   138,   112,\n",
      "        22142, 10688,   112,   140,   102])\n",
      "\n",
      "        segments_tensorÔºötensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "        label_tensor   ÔºöNone\n",
      "\n",
      "        --------------------\n",
      "\n",
      "        [ÈÇÑÂéü tokens_tensors]\n",
      "        [CLS] @ mad ##dow well the far - right patriot that assassinat ##es berg ##dahl use the commander and chief instructions as a defense ? # believe ##me [ ' believe ##me ' ] [SEP]\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "def see_set(t_set):\n",
    "    for i in range(100):\n",
    "        # ÈÅ∏ÊìáÁ¨¨‰∏ÄÂÄãÊ®£Êú¨\n",
    "        sample_idx = i\n",
    "\n",
    "        # Â∞áÂéüÂßãÊñáÊú¨ÊãøÂá∫ÂÅöÊØîËºÉ\n",
    "        text_id, text_a, hashtags, label = t_set.df.iloc[sample_idx].values\n",
    "\n",
    "        # Âà©Áî®ÂâõÂâõÂª∫Á´ãÁöÑ Dataset ÂèñÂá∫ËΩâÊèõÂæåÁöÑ id tensors\n",
    "        tokens_tensor, segments_tensor, label_tensor = t_set[sample_idx]\n",
    "\n",
    "        # Â∞á tokens_tensor ÈÇÑÂéüÊàêÊñáÊú¨\n",
    "        tokens = tokenizer.convert_ids_to_tokens(tokens_tensor.tolist())\n",
    "        combined_text = \" \".join(tokens)\n",
    "\n",
    "        # Ê∏≤ÊüìÂâçÂæåÂ∑ÆÁï∞ÔºåÊØ´ÁÑ°ÂèçÊáâÂ∞±ÊòØÂÄã print„ÄÇÂèØ‰ª•Áõ¥Êé•ÁúãËº∏Âá∫ÁµêÊûú\n",
    "        print(f\"\"\"[ÂéüÂßãÊñáÊú¨]\n",
    "        Âè•Â≠ê 1Ôºö{text_a}\n",
    "        ÂàÜÈ°û  Ôºö{label}\n",
    "\n",
    "        --------------------\n",
    "\n",
    "        [Dataset ÂõûÂÇ≥ÁöÑ tensors]\n",
    "        tokens_tensor  Ôºö{tokens_tensor}\n",
    "\n",
    "        segments_tensorÔºö{segments_tensor}\n",
    "\n",
    "        label_tensor   Ôºö{label_tensor}\n",
    "\n",
    "        --------------------\n",
    "\n",
    "        [ÈÇÑÂéü tokens_tensors]\n",
    "        {combined_text}\n",
    "        \"\"\")\n",
    "\n",
    "see_set(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ÂØ¶‰ΩúÂèØ‰ª•‰∏ÄÊ¨°ÂõûÂÇ≥‰∏ÄÂÄã mini-batch ÁöÑ DataLoader\n",
    "ÈÄôÂÄã DataLoader ÂêÉÊàëÂÄë‰∏äÈù¢ÂÆöÁæ©ÁöÑ `Dataset`Ôºå\n",
    "ÂõûÂÇ≥Ë®ìÁ∑¥ BERT ÊôÇÊúÉÈúÄË¶ÅÁöÑ 4 ÂÄã tensorsÔºö\n",
    "- tokens_tensors  : (batch_size, max_seq_len_in_batch)\n",
    "- segments_tensors: (batch_size, max_seq_len_in_batch)\n",
    "- masks_tensors   : (batch_size, max_seq_len_in_batch)\n",
    "- label_ids       : (batch_size)\n",
    "\"\"\"\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# ÈÄôÂÄãÂáΩÂºèÁöÑËº∏ÂÖ• `samples` ÊòØ‰∏ÄÂÄã listÔºåË£°È†≠ÁöÑÊØèÂÄã element ÈÉΩÊòØ\n",
    "# ÂâõÂâõÂÆöÁæ©ÁöÑ `Dataset` ÂõûÂÇ≥ÁöÑ‰∏ÄÂÄãÊ®£Êú¨ÔºåÊØèÂÄãÊ®£Êú¨ÈÉΩÂåÖÂê´ 3 tensorsÔºö\n",
    "# - tokens_tensor\n",
    "# - segments_tensor\n",
    "# - label_tensor\n",
    "# ÂÆÉÊúÉÂ∞çÂâçÂÖ©ÂÄã tensors ‰Ωú zero paddingÔºå‰∏¶Áî¢ÁîüÂâçÈù¢Ë™™ÊòéÈÅéÁöÑ masks_tensors\n",
    "\n",
    "\n",
    "def create_mini_batch(samples):\n",
    "    tokens_tensors = [s[0] for s in samples]\n",
    "    segments_tensors = [s[1] for s in samples]\n",
    "    # Ê∏¨Ë©¶ÈõÜÊúâ labels\n",
    "    if samples[0][2] is not None:\n",
    "        label_ids = torch.stack([s[2] for s in samples])\n",
    "    else:\n",
    "        label_ids = None\n",
    "    \n",
    "    # zero pad Âà∞Âêå‰∏ÄÂ∫èÂàóÈï∑Â∫¶\n",
    "    tokens_tensors = pad_sequence(tokens_tensors, \n",
    "                                  batch_first=True)\n",
    "    segments_tensors = pad_sequence(segments_tensors, \n",
    "                                    batch_first=True)\n",
    "    \n",
    "    # attention masksÔºåÂ∞á tokens_tensors Ë£°È†≠‰∏çÁÇ∫ zero padding\n",
    "    # ÁöÑ‰ΩçÁΩÆË®≠ÁÇ∫ 1 ËÆì BERT Âè™ÈóúÊ≥®ÈÄô‰∫õ‰ΩçÁΩÆÁöÑ tokens\n",
    "    masks_tensors = torch.zeros(tokens_tensors.shape, \n",
    "                                dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(\n",
    "        tokens_tensors != 0, 1)\n",
    "    \n",
    "    return tokens_tensors, segments_tensors, masks_tensors, label_ids\n",
    "\n",
    "\n",
    "# ÂàùÂßãÂåñ‰∏ÄÂÄãÊØèÊ¨°ÂõûÂÇ≥ 64 ÂÄãË®ìÁ∑¥Ê®£Êú¨ÁöÑ DataLoader\n",
    "# Âà©Áî® `collate_fn` Â∞á list of samples Âêà‰ΩµÊàê‰∏ÄÂÄã mini-batch ÊòØÈóúÈçµ\n",
    "BATCH_SIZE = 4\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, \n",
    "                         collate_fn=create_mini_batch)\n",
    "\n",
    "# testloader = DataLoader(testset, batch_size=BATCH_SIZE, \n",
    "#                          collate_fn=create_mini_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tokens_tensors.shape   = torch.Size([4, 66]) \n",
      "tensor([[   101,  11227,  10488,  10404,    107,  33989,  10525,  10125,    108,\n",
      "          52539,  64415,  20452,    107,  14650,  10346,  10102,  84202,  33492,\n",
      "            119,  10707,  10311,  10564,    119,    119,    119,    119,  10203,\n",
      "            112,    161,    138,    112,    161,    156,    143,    158,    145,\n",
      "            150,    143,    162,    112,    140,    102,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0],\n",
      "        [   101,    137,  14319,  22192,  10416,  10146,  11312,  11811,    117,\n",
      "          29104,  10127,  30728,  10114,    108,  12487,  44885,  12096,  10103,\n",
      "          10228,    119,  11523,    143,    108,  29104,  33667,  12207,    119,\n",
      "            108,  30047,    138,    112,    148,    160,    147,    147,    158,\n",
      "            160,    147,    161,    161,    112,    117,    112,    162,    160,\n",
      "            163,    155,    158,    154,    147,    149,    143,    145,    167,\n",
      "            112,    117,    112,    145,    156,    156,    112,    140,    102,\n",
      "              0,      0,      0],\n",
      "        [   101,  11628,  49351,  10112,  10127,  16104,  15995,  17439,  10911,\n",
      "         106695, 106695, 106695,    138,    140,    102,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0],\n",
      "        [   101,    137,  87621,  38644,    137,  10103,  10499,  23691,  46713,\n",
      "          10273,  29263,  10661,  10139,  10103,  11146,  10573,  31200,    119,\n",
      "          11523,  16446,    106,  13645,  83330,  56493,    108,  39253,  88635,\n",
      "          10261,    108,  18418,  14547,  17745,  34238,  10163,  12050,    106,\n",
      "            106,    138,    112,    143,    163,    162,    150,    147,    156,\n",
      "            162,    151,    145,    112,    117,    112,    154,    143,    163,\n",
      "            149,    150,    157,    163,    162,    154,    157,    163,    146,\n",
      "            112,    140,    102]])\n",
      "------------------------\n",
      "segments_tensors.shape = torch.Size([4, 66])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "------------------------\n",
      "masks_tensors.shape    = torch.Size([4, 66])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "------------------------\n",
      "label_ids.shape        = torch.Size([4])\n",
      "tensor([1, 5, 3, 4])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = next(iter(trainloader))\n",
    "# data = next(iter(testloader))\n",
    "\n",
    "tokens_tensors, segments_tensors, \\\n",
    "    masks_tensors, label_ids = data\n",
    "\n",
    "print(f\"\"\"\n",
    "tokens_tensors.shape   = {tokens_tensors.shape} \n",
    "{tokens_tensors}\n",
    "------------------------\n",
    "segments_tensors.shape = {segments_tensors.shape}\n",
    "{segments_tensors}\n",
    "------------------------\n",
    "masks_tensors.shape    = {masks_tensors.shape}\n",
    "{masks_tensors}\n",
    "------------------------\n",
    "label_ids.shape        = {label_ids.shape}\n",
    "{label_ids}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name            module\n",
      "----------------------\n",
      "bert:embeddings\n",
      "bert:encoder\n",
      "bert:pooler\n",
      "dropout         Dropout(p=0.1, inplace=False)\n",
      "123\n",
      "classifier      Linear(in_features=768, out_features=8, bias=True)\n",
      "123\n"
     ]
    }
   ],
   "source": [
    "# ËºâÂÖ•‰∏ÄÂÄãÂèØ‰ª•ÂÅö‰∏≠ÊñáÂ§öÂàÜÈ°û‰ªªÂãôÁöÑÊ®°ÂûãÔºån_class = 3\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "# PRETRAINED_MODEL_NAME = 'bert-base-uncased'\n",
    "PRETRAINED_MODEL_NAME = 'bert-base-multilingual-uncased'\n",
    "\n",
    "NUM_LABELS = 8\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# model = pickle.load(open('model.pkl', 'rb'))\n",
    "\n",
    "clear_output()\n",
    "\n",
    "# high-level È°ØÁ§∫Ê≠§Ê®°ÂûãË£°ÁöÑ modules\n",
    "print(\"\"\"\n",
    "name            module\n",
    "----------------------\"\"\")\n",
    "for name, module in model.named_children():\n",
    "    if name == \"bert\":\n",
    "        for n, _ in module.named_children():\n",
    "            print(f\"{name}:{n}\")\n",
    "    else:\n",
    "        print(\"{:15} {}\".format(name, module))\n",
    "        print (123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ÂÆöÁæ©‰∏ÄÂÄãÂèØ‰ª•ÈáùÂ∞çÁâπÂÆö DataLoader ÂèñÂæóÊ®°ÂûãÈ†êÊ∏¨ÁµêÊûú‰ª•ÂèäÂàÜÈ°ûÊ∫ñÁ¢∫Â∫¶ÁöÑÂáΩÂºè\n",
    "‰πãÂæå‰πüÂèØ‰ª•Áî®‰æÜÁîüÊàê‰∏äÂÇ≥Âà∞ Kaggle Á´∂Ë≥ΩÁöÑÈ†êÊ∏¨ÁµêÊûú\n",
    "\"\"\"\n",
    "\n",
    "# torch.cuda.set_device(2) # ÈÄôË°åÂä†‰∫Ü‰πüËÉΩË∑ë\n",
    "\n",
    "\n",
    "def get_predictions(model, dataloader, compute_acc=False):\n",
    "    predictions = None\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    jindu = 0\n",
    "      \n",
    "    with torch.no_grad():\n",
    "        # ÈÅçÂ∑°Êï¥ÂÄãË≥áÊñôÈõÜ\n",
    "        for data in dataloader:\n",
    "            # Â∞áÊâÄÊúâ tensors ÁßªÂà∞ GPU ‰∏ä\n",
    "            if next(model.parameters()).is_cuda:\n",
    "                data = [t.to(\"cuda:0\") for t in data if t is not None]\n",
    "            \n",
    "            \n",
    "            # Âà•ÂøòË®òÂâç 3 ÂÄã tensors ÂàÜÂà•ÁÇ∫ tokens, segments ‰ª•Âèä masks\n",
    "            # ‰∏îÂº∑ÁÉàÂª∫Ë≠∞Âú®Â∞áÈÄô‰∫õ tensors ‰∏üÂÖ• `model` ÊôÇÊåáÂÆöÂ∞çÊáâÁöÑÂèÉÊï∏ÂêçÁ®±\n",
    "            tokens_tensors, segments_tensors, masks_tensors = data[:3]\n",
    "            outputs = model(input_ids=tokens_tensors, \n",
    "                            token_type_ids=segments_tensors, \n",
    "                            attention_mask=masks_tensors)\n",
    "            \n",
    "            logits = outputs[0]\n",
    "            _, pred = torch.max(logits.data, 1)\n",
    "            \n",
    "            # Áî®‰æÜË®àÁÆóË®ìÁ∑¥ÈõÜÁöÑÂàÜÈ°ûÊ∫ñÁ¢∫Áéá\n",
    "            if compute_acc:\n",
    "                labels = data[3]\n",
    "                total += labels.size(0)\n",
    "                correct += (pred == labels).sum().item()\n",
    "                \n",
    "            # Â∞áÁï∂Ââç batch Ë®òÈåÑ‰∏ã‰æÜ\n",
    "            if predictions is None:\n",
    "                predictions = pred\n",
    "            else:\n",
    "                predictions = torch.cat((predictions, pred))\n",
    "            \n",
    "            jindu += BATCH_SIZE\n",
    "            if jindu%1000==0:\n",
    "                print(jindu, '-----')\n",
    "    \n",
    "    if compute_acc:\n",
    "        acc = correct / total\n",
    "        print(acc)\n",
    "        return predictions, acc\n",
    "    return predictions\n",
    "    \n",
    "# # ËÆìÊ®°ÂûãË∑ëÂú® GPU ‰∏ä‰∏¶ÂèñÂæóË®ìÁ∑¥ÈõÜÁöÑÂàÜÈ°ûÊ∫ñÁ¢∫Áéá\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(\"device:\", device)\n",
    "# model = model.to(device)\n",
    "# test, acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "# print(\"classification acc:\", acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Êï¥ÂÄãÂàÜÈ°ûÊ®°ÂûãÁöÑÂèÉÊï∏ÈáèÔºö168221192\n",
      "Á∑öÊÄßÂàÜÈ°ûÂô®ÁöÑÂèÉÊï∏ÈáèÔºö6152\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_learnable_params(module):\n",
    "    return [p for p in module.parameters() if p.requires_grad]\n",
    "     \n",
    "model_params = get_learnable_params(model)\n",
    "clf_params = get_learnable_params(model.classifier)\n",
    "\n",
    "print(f\"\"\"\n",
    "Êï¥ÂÄãÂàÜÈ°ûÊ®°ÂûãÁöÑÂèÉÊï∏ÈáèÔºö{sum(p.numel() for p in model_params)}\n",
    "Á∑öÊÄßÂàÜÈ°ûÂô®ÁöÑÂèÉÊï∏ÈáèÔºö{sum(p.numel() for p in clf_params)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec 21 22:10:35 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 107...  Off  | 00000000:03:00.0 Off |                  N/A |\n",
      "|  0%   40C    P8     7W / 180W |      2MiB /  8119MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce RTX 208...  Off  | 00000000:0A:00.0 Off |                  N/A |\n",
      "|  0%   50C    P2    76W / 250W |   4692MiB / 11016MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce RTX 208...  Off  | 00000000:0B:00.0 Off |                  N/A |\n",
      "| 30%   39C    P8    20W / 250W |      3MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    1   N/A  N/A     21430      C   /usr/bin/python3                 4689MiB |\n",
      "+-----------------------------------------------------------------------------+\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# torch.cuda.set_device(2)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") ## specify the GPU id's, GPU id's start from 0.\n",
    "# model= nn.DataParallel(model,device_ids = [0, 1])\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "! nvidia-smi\n",
    "\n",
    "# Ë®ìÁ∑¥Ê®°Âºè\n",
    "model.train()\n",
    "\n",
    "# ‰ΩøÁî® Adam Optim Êõ¥Êñ∞Êï¥ÂÄãÂàÜÈ°ûÊ®°ÂûãÁöÑÂèÉÊï∏\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "EPOCHS = 6  # Âπ∏ÈÅãÊï∏Â≠ó\n",
    "jindu = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    for data in trainloader:\n",
    "        tokens_tensors, segments_tensors, \\\n",
    "        masks_tensors, labels = [t.to(device) for t in data]\n",
    "        # Â∞áÂèÉÊï∏Ê¢ØÂ∫¶Ê≠∏Èõ∂\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # forward pass\n",
    "        outputs = model(input_ids=tokens_tensors, \n",
    "                        token_type_ids=segments_tensors, \n",
    "                        attention_mask=masks_tensors, \n",
    "                        labels=labels)\n",
    "        loss = outputs[0]\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Á¥ÄÈåÑÁï∂Ââç batch loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        jindu += BATCH_SIZE\n",
    "        if jindu%1000==0:\n",
    "            print (jindu)\n",
    "\n",
    "    # Ë®àÁÆóÂàÜÈ°ûÊ∫ñÁ¢∫Áéá\n",
    "    _, acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "\n",
    "    print('[epoch %d] loss: %.3f, acc: %.3f' %\n",
    "          (epoch + 1, running_loss, acc))\n",
    "    \n",
    "    pickle.dump(model, open('model_emoji_multilingual_{}.pkl'.format(epoch), 'wb'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(model, open('model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Â∞çÊñ∞Ê®£Êú¨ÂÅöÊé®Ë´ñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Âª∫Á´ãÊ∏¨Ë©¶ÈõÜ„ÄÇÈÄôÈÇäÊàëÂÄëÂèØ‰ª•Áî®Ë∑üË®ìÁ∑¥ÊôÇ‰∏çÂêåÁöÑ batch_sizeÔºåÁúã‰Ω† GPU Â§öÂ§ß\n",
    "testset = Dataset(\"test\", tokenizer=tokenizer)\n",
    "testloader = DataLoader(testset, batch_size=4, \n",
    "                        collate_fn=create_mini_batch)\n",
    "\n",
    "# Áî®ÂàÜÈ°ûÊ®°ÂûãÈ†êÊ∏¨Ê∏¨Ë©¶ÈõÜ\n",
    "predictions = get_predictions(model, testloader)\n",
    "\n",
    "# Áî®‰æÜÂ∞áÈ†êÊ∏¨ÁöÑ label id ËΩâÂõû label ÊñáÂ≠ó\n",
    "index_map = {v: k for k, v in testset.label_map.items()}\n",
    "\n",
    "# ÁîüÊàê Kaggle Áπ≥‰∫§Ê™îÊ°à\n",
    "df = pd.DataFrame({\"emotion\": predictions.tolist()})\n",
    "df['emotion'] = df.Category.apply(lambda x: index_map[x])\n",
    "df_pred = pd.concat([testset.df.loc[:, [\"ID\"]], \n",
    "                          df.loc[:, 'Category']], axis=1)\n",
    "df_pred.to_csv('bert_submission.csv', index=False)\n",
    "df_pred.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
