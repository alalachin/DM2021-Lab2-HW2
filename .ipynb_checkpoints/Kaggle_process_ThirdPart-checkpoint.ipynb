{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "\n",
    "# with zipfile.ZipFile('dm2021-lab2-hw2.zip', 'r') as zip_ref:\n",
    "#     zip_ref.extractall('./kaggle_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_tweets = pd.read_json(\"./kaggle_data/tweets_DM.json\", lines=True)\n",
    "df_tot = pd.read_csv(\"./kaggle_data/data_identification.csv\")\n",
    "df_emotion = pd.read_csv(\"./kaggle_data/emotion.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observe dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index(['_score', '_index', '_source', '_crawldate', '_type'], dtype='object')\n",
      "<class 'pandas.core.series.Series'> (1867535,)\n",
      "<class 'dict'> dict_keys(['tweet'])\n",
      "<class 'dict'> dict_keys(['hashtags', 'tweet_id', 'text'])\n",
      "{'hashtags': ['Snapchat'], 'tweet_id': '0x376b20', 'text': 'People who post \"add me on #Snapchat\" must be dehydrated. Cuz man.... that\\'s <LH>'}\n",
      "\n",
      "Index(['tweet_id', 'identification'], dtype='object')\n",
      "\n",
      "Index(['tweet_id', 'emotion'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# df_tweets\n",
    "print (type(df_tweets))\n",
    "print (df_tweets.columns)\n",
    "print (type(df_tweets['_source']), df_tweets['_source'].shape)\n",
    "print (type(df_tweets['_source'][0]), df_tweets['_source'][0].keys())\n",
    "print (type(df_tweets['_source'][0]['tweet']), df_tweets['_source'][0]['tweet'].keys())\n",
    "print (df_tweets['_source'][0]['tweet'])\n",
    "print ()\n",
    "\n",
    "# df_tot (train or test)\n",
    "print (df_tot.columns)\n",
    "print ()\n",
    "\n",
    "# df_emotion\n",
    "print (df_emotion.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emotion</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>39867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anticipation</th>\n",
       "      <td>248935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>139101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>63999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>516017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>193437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>48729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trust</th>\n",
       "      <td>205478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id\n",
       "emotion               \n",
       "anger            39867\n",
       "anticipation    248935\n",
       "disgust         139101\n",
       "fear             63999\n",
       "joy             516017\n",
       "sadness         193437\n",
       "surprise         48729\n",
       "trust           205478"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_emotion.groupby('emotion').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are totally 1867535 tweets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'hashtags': ['Snapchat'],\n",
       " 'tweet_id': '0x376b20',\n",
       " 'text': 'People who post \"add me on #Snapchat\" must be dehydrated. Cuz man.... that\\'s <LH>'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"There are totally\", len(df_tweets['_source']), \"tweets\")\n",
    "df_tweets['_source'][0]['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "hashtags = []\n",
    "tweets = []\n",
    "# for i in range(df_tweets.shape[0]):\n",
    "for i in range(100):\n",
    "    twid = df_tweets['_source'][i]['tweet']['tweet_id']\n",
    "    tot = df_tot.loc[df_tot['tweet_id']==twid]\n",
    "    if tot['identification'].to_list()[-1]=='train':\n",
    "#         emo = df_emotion.loc[df_emotion['tweet_id']==twid]['emotion']\n",
    "#         print (emo, (df_tweets['_source'][i]['tweet']['hashtags']))\n",
    "        print (df_tweets['_source'][i]['tweet']['text'])\n",
    "        print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make data_dict and emoji_list\n",
    "\n",
    "data_dict \\[ tweet_id \\] = \\[ text, hashtags, emotion\\ ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "import emoji\n",
    "import pickle\n",
    "\n",
    "reg_tokenizer = RegexpTokenizer(\"\\w+[\\.|\\']\\w+|\\w+|\\#\\w+|\\@\\w+|\\u2764|[\\U0001F600-\\U0001F64F]|[\\U0001F300-\\U0001F5FF]|[\\U0001F680-\\U0001F6FF]|[\\U0001F1E0-\\U0001F1FF]\")\n",
    "\n",
    "\n",
    "def extract_emojis(s):\n",
    "    return ''.join(c for c in s if c in emoji.UNICODE_EMOJI['en'])\n",
    "\n",
    "emoji_set = set()\n",
    "data_dict = {}\n",
    "for i in range(len(df_tweets)):\n",
    "    text = df_tweets['_source'][i]['tweet']['text']\n",
    "    text = text.replace('<LH>', '')\n",
    "    emoji_set = emoji_set.union(set(extract_emojis(text)))\n",
    "    tid = df_tweets['_source'][i]['tweet']['tweet_id']\n",
    "    tags = df_tweets['_source'][i]['tweet']['hashtags']\n",
    "    data_dict[tid] = [text, tags, ''] # [text, hashtags, emo]\n",
    "\n",
    "    # print (text)\n",
    "#     text_list_small.append(text)\n",
    "#     id_list_small.append(tid)\n",
    "#     text = tokenizer.tokenize(text)\n",
    "\n",
    "emoji_set = list(emoji_set)\n",
    "pickle.dump(emoji_set, open('emoji_list.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['People who post \"add me on #Snapchat\" must be dehydrated. Cuz man.... that\\'s ',\n",
       " ['Snapchat'],\n",
       " '']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict['0x376b20']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make data table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for _, row in df_emotion.iterrows():\n",
    "    if row.tweet_id not in data_dict:\n",
    "        print (row.tweet_id)\n",
    "    data_dict[row.tweet_id][-1] = row.emotion\n",
    "    \n",
    "    \n",
    "df_data = pd.DataFrame.from_dict(data_dict, orient='index', columns=['text', 'hashtags', 'emotion'])\n",
    "df_data['ID'] = df_data.index\n",
    "df_data = df_data[['ID', 'text', 'hashtags', 'emotion']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0x376b20</th>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x2d5350</th>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x28b412</th>\n",
       "      <td>0x28b412</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "      <td>[bibleverse]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x1cd5b0</th>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha ğŸ˜‚ğŸ˜‚ğŸ˜‚</td>\n",
       "      <td>[]</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x2de201</th>\n",
       "      <td>0x2de201</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID                                               text  \\\n",
       "0x376b20  0x376b20  People who post \"add me on #Snapchat\" must be ...   \n",
       "0x2d5350  0x2d5350  @brianklaas As we see, Trump is dangerous to #...   \n",
       "0x28b412  0x28b412  Confident of your obedience, I write to you, k...   \n",
       "0x1cd5b0  0x1cd5b0                    Now ISSA is stalking Tasha ğŸ˜‚ğŸ˜‚ğŸ˜‚    \n",
       "0x2de201  0x2de201  \"Trust is not the same as faith. A friend is s...   \n",
       "\n",
       "                               hashtags       emotion  \n",
       "0x376b20                     [Snapchat]  anticipation  \n",
       "0x2d5350  [freepress, TrumpLegacy, CNN]       sadness  \n",
       "0x28b412                   [bibleverse]                \n",
       "0x1cd5b0                             []          fear  \n",
       "0x2de201                             []                "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0x376b20</th>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x2d5350</th>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x1cd5b0</th>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha ğŸ˜‚ğŸ˜‚ğŸ˜‚</td>\n",
       "      <td>[]</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x1d755c</th>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x2c91a8</th>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>Still waiting on those supplies Liscus.</td>\n",
       "      <td>[]</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID                                               text  \\\n",
       "0x376b20  0x376b20  People who post \"add me on #Snapchat\" must be ...   \n",
       "0x2d5350  0x2d5350  @brianklaas As we see, Trump is dangerous to #...   \n",
       "0x1cd5b0  0x1cd5b0                    Now ISSA is stalking Tasha ğŸ˜‚ğŸ˜‚ğŸ˜‚    \n",
       "0x1d755c  0x1d755c  @RISKshow @TheKevinAllison Thx for the BEST TI...   \n",
       "0x2c91a8  0x2c91a8           Still waiting on those supplies Liscus.    \n",
       "\n",
       "                               hashtags       emotion  \n",
       "0x376b20                     [Snapchat]  anticipation  \n",
       "0x2d5350  [freepress, TrumpLegacy, CNN]       sadness  \n",
       "0x1cd5b0                             []          fear  \n",
       "0x1d755c      [authentic, LaughOutLoud]           joy  \n",
       "0x2c91a8                             []  anticipation  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## make training dataset\n",
    "df_data.loc[df_data['emotion']!=''].to_csv('train_w_hashtag.tsv', sep='\\t', index=False)\n",
    "df_data.loc[df_data['emotion']!=''].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0x28b412</th>\n",
       "      <td>0x28b412</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "      <td>[bibleverse]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x2de201</th>\n",
       "      <td>0x2de201</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x218443</th>\n",
       "      <td>0x218443</td>\n",
       "      <td>When do you have enough ? When are you satisfi...</td>\n",
       "      <td>[materialism, money, possessions]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID                                               text  \\\n",
       "0x28b412  0x28b412  Confident of your obedience, I write to you, k...   \n",
       "0x2de201  0x2de201  \"Trust is not the same as faith. A friend is s...   \n",
       "0x218443  0x218443  When do you have enough ? When are you satisfi...   \n",
       "\n",
       "                                   hashtags emotion  \n",
       "0x28b412                       [bibleverse]          \n",
       "0x2de201                                 []          \n",
       "0x218443  [materialism, money, possessions]          "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## make testing dataset\n",
    "df_data.loc[df_data['emotion']==''].to_csv('test.tsv', sep='\\t', index=False)\n",
    "df_data.loc[df_data['emotion']==''].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenize and vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lihsuan/.local/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:  (1455563, 1000)\n",
      "y_train.shape:  (1455563,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "reg_tokenizer = RegexpTokenizer(\"\\w+[\\.|\\']\\w+|\\w+|\\#\\w+|\\@\\w+|\\u2764|[\\U0001F600-\\U0001F64F]|[\\U0001F300-\\U0001F5FF]|[\\U0001F680-\\U0001F6FF]|[\\U0001F1E0-\\U0001F1FF]\")\n",
    "tf_idf = TfidfVectorizer(max_features=1000, tokenizer=reg_tokenizer.tokenize)\n",
    "tf_idf.fit(df_data['text'])\n",
    "\n",
    "# for a classificaiton problem, you need to provide both training & testing data\n",
    "X = tf_idf.transform(df_data.loc[df_data['emotion']!=\"\"]['text'])\n",
    "y = df_data.loc[df_data['emotion']!=\"\"]['emotion']\n",
    "\n",
    "## take a look at data dimension :)\n",
    "print('X_train.shape: ', X.shape)\n",
    "print('y_train.shape: ', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# decomposition with SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=800, n_iter=200, random_state=42)\n",
    "X = svd.fit_transform(X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# confusion matrix plotting module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, title='Confusion matrix',\n",
    "                          cmap=sns.cubehelix_palette(as_cmap=True)):\n",
    "\n",
    "    classes.sort()\n",
    "    tick_marks = np.arange(len(classes))    \n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           xticklabels = classes,\n",
    "           yticklabels = classes,\n",
    "           title = title,\n",
    "           xlabel = 'True label',\n",
    "           ylabel = 'Predicted label')\n",
    "\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    ylim_top = len(classes) - 0.5\n",
    "    plt.ylim([ylim_top, -.5])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1st model: Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "## build DecisionTree model\n",
    "DT_model = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "## training!\n",
    "DT_model = DT_model.fit(X_train, y_train)\n",
    "\n",
    "## predict!\n",
    "y_train_pred = DT_model.predict(X_train)\n",
    "y_test_pred = DT_model.predict(X_test)\n",
    "\n",
    "acc_train = accuracy_score(y_true=y_train, y_pred=y_train_pred)\n",
    "acc_test = accuracy_score(y_true=y_test, y_pred=y_test_pred)\n",
    "\n",
    "print('training accuracy: {}'.format(round(acc_train, 2)))\n",
    "print('testing accuracy: {}'.format(round(acc_test, 2)))\n",
    "\n",
    "DT_cm = confusion_matrix(y_true=y_test, y_pred=y_test_pred) \n",
    "print(classification_report(y_true=y_test, y_pred=y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tags = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'sadness', 'surprise', 'trust']\n",
    "plot_confusion_matrix(DT_cm, classes=my_tags, title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2nd model: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "RF_model = RandomForestClassifier(max_depth=200)\n",
    "RF_model = RF_model.fit(X_train, y_train)\n",
    "\n",
    "## predict!\n",
    "y_train_pred = RF_model.predict(X_train)\n",
    "y_test_pred = RF_model.predict(X_test)\n",
    "\n",
    "acc_train = accuracy_score(y_true=y_train, y_pred=y_train_pred)\n",
    "acc_test = accuracy_score(y_true=y_test, y_pred=y_test_pred)\n",
    "\n",
    "print('training accuracy: {}'.format(round(acc_train, 2)))\n",
    "print('testing accuracy: {}'.format(round(acc_test, 2)))\n",
    "\n",
    "RF_cm = confusion_matrix(y_true=y_test, y_pred=y_test_pred) \n",
    "print(classification_report(y_true=y_test, y_pred=y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tags = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'sadness', 'surprise', 'trust']\n",
    "plot_confusion_matrix(RF_cm, classes=my_tags, title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3rd model: Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from IPython.display import clear_output\n",
    "from transformers import BertForMaskedLM\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1.0\n",
      "Name: emotion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "tokenizer.add_tokens(emoji_set)\n",
    "    \n",
    "class Dataset(Dataset):\n",
    "    ## read the preprocessed .tsv and  initailize some parameter\n",
    "    def __init__(self, mode, tokenizer):\n",
    "        assert mode in [\"train\", \"test\"]\n",
    "        self.mode = mode\n",
    "        self.df = pd.read_csv(mode + \"_w_hashtag.tsv\", sep=\"\\t\", lineterminator='\\n').fillna(\"\")\n",
    "        self.len = len(self.df)\n",
    "        self.label_map = {'anger':0, 'anticipation':1, 'disgust':2, 'fear':3, 'joy':4, 'sadness':5, 'surprise':6, 'trust':7}\n",
    "        print (self.df.emotion.value_counts() / len(self.df))\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "        tokenizer.add_tokens(emoji_set)\n",
    "        ## will use bert tokenizer\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    \n",
    "    ## return a training / testing data\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == \"test\":\n",
    "            text_id, text_a, text_hashtag = self.df.iloc[idx, :3].values\n",
    "            label_tensor = None\n",
    "        else:\n",
    "            text_id, text_a, text_hashtag, label = self.df.iloc[idx, :].values\n",
    "            \n",
    "            ## convert the label from text to index\n",
    "            ## in order for making tensor\n",
    "            label_id = self.label_map[label]\n",
    "            label_tensor = torch.tensor(label_id)\n",
    "            \n",
    "        ## create BERT token for a tweet\n",
    "        ## add [SEP]\n",
    "        word_pieces = [\"[CLS]\"]\n",
    "        # tokens_a = self.tokenizer.tokenize(text_a)\n",
    "        text_a = text_a + \"\".join(text_hashtag)\n",
    "        tokens_a = self.tokenizer.tokenize(text_a)\n",
    "        word_pieces += tokens_a + [\"[SEP]\"]\n",
    "        len_a = len(word_pieces)\n",
    "        \n",
    "        ## convert tokens sequence to index sequence\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "        tokens_tensor = torch.tensor(ids)\n",
    "        segments_tensor = torch.tensor([0] * len_a, dtype=torch.long)\n",
    "        \n",
    "        return (tokens_tensor, segments_tensor, label_tensor)\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    \n",
    "# åˆå§‹åŒ–ä¸€å€‹å°ˆé–€è®€å–è¨“ç·´æ¨£æœ¬çš„ Datasetï¼Œä½¿ç”¨ä¸­æ–‡ BERT æ–·è©\n",
    "trainset = Dataset(\"train\", tokenizer=tokenizer)\n",
    "# testset = Dataset(\"test\", tokenizer=tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train set:\n",
    "joy             0.354514\n",
    "anticipation    0.171023\n",
    "trust           0.141167\n",
    "sadness         0.132895\n",
    "disgust         0.095565\n",
    "fear            0.043969\n",
    "surprise        0.033478\n",
    "anger           0.027389"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You don't choose your  ğŸ˜± but you get to choose your #fancy\\r#FancyVsFear\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1366\n",
    "trainset.df.iloc[1366][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0x1d69d5</th>\n",
       "      <td>0x1d69d5</td>\n",
       "      <td>You don't choose your  ğŸ˜± but you get to choose...</td>\n",
       "      <td>[fancy, FancyVsFear]</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID                                               text  \\\n",
       "0x1d69d5  0x1d69d5  You don't choose your  ğŸ˜± but you get to choose...   \n",
       "\n",
       "                      hashtags emotion  \n",
       "0x1d69d5  [fancy, FancyVsFear]    fear  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_emotion.loc[df_emotion['tweet_id']=='0x1d69d5']\n",
    "df_data.loc[df_data['ID']=='0x1d69d5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def see_set(t_set):\n",
    "    for i in range(100):\n",
    "        # é¸æ“‡ç¬¬ä¸€å€‹æ¨£æœ¬\n",
    "        sample_idx = i\n",
    "\n",
    "        # å°‡åŸå§‹æ–‡æœ¬æ‹¿å‡ºåšæ¯”è¼ƒ\n",
    "        text_id, text_a, hashtags, label = t_set.df.iloc[sample_idx].values\n",
    "\n",
    "        # åˆ©ç”¨å‰›å‰›å»ºç«‹çš„ Dataset å–å‡ºè½‰æ›å¾Œçš„ id tensors\n",
    "        tokens_tensor, segments_tensor, label_tensor = t_set[sample_idx]\n",
    "\n",
    "        # å°‡ tokens_tensor é‚„åŸæˆæ–‡æœ¬\n",
    "        tokens = tokenizer.convert_ids_to_tokens(tokens_tensor.tolist())\n",
    "        combined_text = \" \".join(tokens)\n",
    "\n",
    "        # æ¸²æŸ“å‰å¾Œå·®ç•°ï¼Œæ¯«ç„¡åæ‡‰å°±æ˜¯å€‹ printã€‚å¯ä»¥ç›´æ¥çœ‹è¼¸å‡ºçµæœ\n",
    "        print(f\"\"\"[åŸå§‹æ–‡æœ¬]\n",
    "        å¥å­ 1ï¼š{text_a}\n",
    "        åˆ†é¡  ï¼š{label}\n",
    "\n",
    "        --------------------\n",
    "\n",
    "        [Dataset å›å‚³çš„ tensors]\n",
    "        tokens_tensor  ï¼š{tokens_tensor}\n",
    "\n",
    "        segments_tensorï¼š{segments_tensor}\n",
    "\n",
    "        label_tensor   ï¼š{label_tensor}\n",
    "\n",
    "        --------------------\n",
    "\n",
    "        [é‚„åŸ tokens_tensors]\n",
    "        {combined_text}\n",
    "        \"\"\")\n",
    "\n",
    "see_set(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "å¯¦ä½œå¯ä»¥ä¸€æ¬¡å›å‚³ä¸€å€‹ mini-batch çš„ DataLoader\n",
    "é€™å€‹ DataLoader åƒæˆ‘å€‘ä¸Šé¢å®šç¾©çš„ `Dataset`ï¼Œ\n",
    "å›å‚³è¨“ç·´ BERT æ™‚æœƒéœ€è¦çš„ 4 å€‹ tensorsï¼š\n",
    "- tokens_tensors  : (batch_size, max_seq_len_in_batch)\n",
    "- segments_tensors: (batch_size, max_seq_len_in_batch)\n",
    "- masks_tensors   : (batch_size, max_seq_len_in_batch)\n",
    "- label_ids       : (batch_size)\n",
    "\"\"\"\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# é€™å€‹å‡½å¼çš„è¼¸å…¥ `samples` æ˜¯ä¸€å€‹ listï¼Œè£¡é ­çš„æ¯å€‹ element éƒ½æ˜¯\n",
    "# å‰›å‰›å®šç¾©çš„ `Dataset` å›å‚³çš„ä¸€å€‹æ¨£æœ¬ï¼Œæ¯å€‹æ¨£æœ¬éƒ½åŒ…å« 3 tensorsï¼š\n",
    "# - tokens_tensor\n",
    "# - segments_tensor\n",
    "# - label_tensor\n",
    "# å®ƒæœƒå°å‰å…©å€‹ tensors ä½œ zero paddingï¼Œä¸¦ç”¢ç”Ÿå‰é¢èªªæ˜éçš„ masks_tensors\n",
    "\n",
    "\n",
    "def create_mini_batch(samples):\n",
    "    tokens_tensors = [s[0] for s in samples]\n",
    "    segments_tensors = [s[1] for s in samples]\n",
    "    # æ¸¬è©¦é›†æœ‰ labels\n",
    "    if samples[0][2] is not None:\n",
    "        label_ids = torch.stack([s[2] for s in samples])\n",
    "    else:\n",
    "        label_ids = None\n",
    "    \n",
    "    # zero pad åˆ°åŒä¸€åºåˆ—é•·åº¦\n",
    "    tokens_tensors = pad_sequence(tokens_tensors, \n",
    "                                  batch_first=True)\n",
    "    segments_tensors = pad_sequence(segments_tensors, \n",
    "                                    batch_first=True)\n",
    "    \n",
    "    # attention masksï¼Œå°‡ tokens_tensors è£¡é ­ä¸ç‚º zero padding\n",
    "    # çš„ä½ç½®è¨­ç‚º 1 è®“ BERT åªé—œæ³¨é€™äº›ä½ç½®çš„ tokens\n",
    "    masks_tensors = torch.zeros(tokens_tensors.shape, \n",
    "                                dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(\n",
    "        tokens_tensors != 0, 1)\n",
    "    \n",
    "    return tokens_tensors, segments_tensors, masks_tensors, label_ids\n",
    "\n",
    "\n",
    "# åˆå§‹åŒ–ä¸€å€‹æ¯æ¬¡å›å‚³ 64 å€‹è¨“ç·´æ¨£æœ¬çš„ DataLoader\n",
    "# åˆ©ç”¨ `collate_fn` å°‡ list of samples åˆä½µæˆä¸€å€‹ mini-batch æ˜¯é—œéµ\n",
    "BATCH_SIZE = 4\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, \n",
    "                         collate_fn=create_mini_batch)\n",
    "\n",
    "# testloader = DataLoader(testset, batch_size=BATCH_SIZE, \n",
    "#                          collate_fn=create_mini_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name            module\n",
      "----------------------\n",
      "bert:embeddings\n",
      "bert:encoder\n",
      "bert:pooler\n",
      "dropout         Dropout(p=0.1, inplace=False)\n",
      "123\n",
      "classifier      Linear(in_features=768, out_features=8, bias=True)\n",
      "123\n"
     ]
    }
   ],
   "source": [
    "# è¼‰å…¥ä¸€å€‹å¯ä»¥åšä¸­æ–‡å¤šåˆ†é¡ä»»å‹™çš„æ¨¡å‹ï¼Œn_class = 3\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "# PRETRAINED_MODEL_NAME = 'bert-base-uncased'\n",
    "PRETRAINED_MODEL_NAME = 'bert-base-multilingual-uncased'\n",
    "\n",
    "NUM_LABELS = 8\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# model = pickle.load(open('model.pkl', 'rb'))\n",
    "\n",
    "clear_output()\n",
    "\n",
    "# high-level é¡¯ç¤ºæ­¤æ¨¡å‹è£¡çš„ modules\n",
    "print(\"\"\"\n",
    "name            module\n",
    "----------------------\"\"\")\n",
    "for name, module in model.named_children():\n",
    "    if name == \"bert\":\n",
    "        for n, _ in module.named_children():\n",
    "            print(f\"{name}:{n}\")\n",
    "    else:\n",
    "        print(\"{:15} {}\".format(name, module))\n",
    "        print (123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "å®šç¾©ä¸€å€‹å¯ä»¥é‡å°ç‰¹å®š DataLoader å–å¾—æ¨¡å‹é æ¸¬çµæœä»¥åŠåˆ†é¡æº–ç¢ºåº¦çš„å‡½å¼\n",
    "ä¹‹å¾Œä¹Ÿå¯ä»¥ç”¨ä¾†ç”Ÿæˆä¸Šå‚³åˆ° Kaggle ç«¶è³½çš„é æ¸¬çµæœ\n",
    "\"\"\"\n",
    "\n",
    "# torch.cuda.set_device(2) # é€™è¡ŒåŠ äº†ä¹Ÿèƒ½è·‘\n",
    "\n",
    "\n",
    "def get_predictions(model, dataloader, compute_acc=False):\n",
    "    predictions = None\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    jindu = 0\n",
    "      \n",
    "    with torch.no_grad():\n",
    "        # éå·¡æ•´å€‹è³‡æ–™é›†\n",
    "        for data in dataloader:\n",
    "            # å°‡æ‰€æœ‰ tensors ç§»åˆ° GPU ä¸Š\n",
    "            if next(model.parameters()).is_cuda:\n",
    "                data = [t.to(\"cuda:0\") for t in data if t is not None]\n",
    "            \n",
    "            \n",
    "            # åˆ¥å¿˜è¨˜å‰ 3 å€‹ tensors åˆ†åˆ¥ç‚º tokens, segments ä»¥åŠ masks\n",
    "            # ä¸”å¼·çƒˆå»ºè­°åœ¨å°‡é€™äº› tensors ä¸Ÿå…¥ `model` æ™‚æŒ‡å®šå°æ‡‰çš„åƒæ•¸åç¨±\n",
    "            tokens_tensors, segments_tensors, masks_tensors = data[:3]\n",
    "            outputs = model(input_ids=tokens_tensors, \n",
    "                            token_type_ids=segments_tensors, \n",
    "                            attention_mask=masks_tensors)\n",
    "            \n",
    "            logits = outputs[0]\n",
    "            _, pred = torch.max(logits.data, 1)\n",
    "            \n",
    "            # ç”¨ä¾†è¨ˆç®—è¨“ç·´é›†çš„åˆ†é¡æº–ç¢ºç‡\n",
    "            if compute_acc:\n",
    "                labels = data[3]\n",
    "                total += labels.size(0)\n",
    "                correct += (pred == labels).sum().item()\n",
    "                \n",
    "            # å°‡ç•¶å‰ batch è¨˜éŒ„ä¸‹ä¾†\n",
    "            if predictions is None:\n",
    "                predictions = pred\n",
    "            else:\n",
    "                predictions = torch.cat((predictions, pred))\n",
    "            \n",
    "            jindu += BATCH_SIZE\n",
    "            if jindu%1000==0:\n",
    "                print(jindu, '-----')\n",
    "    \n",
    "    if compute_acc:\n",
    "        acc = correct / total\n",
    "        print(acc)\n",
    "        return predictions, acc\n",
    "    return predictions\n",
    "    \n",
    "# # è®“æ¨¡å‹è·‘åœ¨ GPU ä¸Šä¸¦å–å¾—è¨“ç·´é›†çš„åˆ†é¡æº–ç¢ºç‡\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(\"device:\", device)\n",
    "# model = model.to(device)\n",
    "# test, acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "# print(\"classification acc:\", acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "æ•´å€‹åˆ†é¡æ¨¡å‹çš„åƒæ•¸é‡ï¼š168221192\n",
      "ç·šæ€§åˆ†é¡å™¨çš„åƒæ•¸é‡ï¼š6152\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_learnable_params(module):\n",
    "    return [p for p in module.parameters() if p.requires_grad]\n",
    "     \n",
    "model_params = get_learnable_params(model)\n",
    "clf_params = get_learnable_params(model.classifier)\n",
    "\n",
    "print(f\"\"\"\n",
    "æ•´å€‹åˆ†é¡æ¨¡å‹çš„åƒæ•¸é‡ï¼š{sum(p.numel() for p in model_params)}\n",
    "ç·šæ€§åˆ†é¡å™¨çš„åƒæ•¸é‡ï¼š{sum(p.numel() for p in clf_params)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec 21 22:10:35 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 107...  Off  | 00000000:03:00.0 Off |                  N/A |\n",
      "|  0%   40C    P8     7W / 180W |      2MiB /  8119MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce RTX 208...  Off  | 00000000:0A:00.0 Off |                  N/A |\n",
      "|  0%   50C    P2    76W / 250W |   4692MiB / 11016MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce RTX 208...  Off  | 00000000:0B:00.0 Off |                  N/A |\n",
      "| 30%   39C    P8    20W / 250W |      3MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    1   N/A  N/A     21430      C   /usr/bin/python3                 4689MiB |\n",
      "+-----------------------------------------------------------------------------+\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# torch.cuda.set_device(2)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") ## specify the GPU id's, GPU id's start from 0.\n",
    "# model= nn.DataParallel(model,device_ids = [0, 1])\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "! nvidia-smi\n",
    "\n",
    "# è¨“ç·´æ¨¡å¼\n",
    "model.train()\n",
    "\n",
    "# ä½¿ç”¨ Adam Optim æ›´æ–°æ•´å€‹åˆ†é¡æ¨¡å‹çš„åƒæ•¸\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "EPOCHS = 6  # å¹¸é‹æ•¸å­—\n",
    "jindu = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    for data in trainloader:\n",
    "        tokens_tensors, segments_tensors, \\\n",
    "        masks_tensors, labels = [t.to(device) for t in data]\n",
    "        # å°‡åƒæ•¸æ¢¯åº¦æ­¸é›¶\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # forward pass\n",
    "        outputs = model(input_ids=tokens_tensors, \n",
    "                        token_type_ids=segments_tensors, \n",
    "                        attention_mask=masks_tensors, \n",
    "                        labels=labels)\n",
    "        loss = outputs[0]\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # ç´€éŒ„ç•¶å‰ batch loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        jindu += BATCH_SIZE\n",
    "        if jindu%1000==0:\n",
    "            print (jindu)\n",
    "\n",
    "    # è¨ˆç®—åˆ†é¡æº–ç¢ºç‡\n",
    "    _, acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "\n",
    "    print('[epoch %d] loss: %.3f, acc: %.3f' %\n",
    "          (epoch + 1, running_loss, acc))\n",
    "    \n",
    "    pickle.dump(model, open('model_emoji_multilingual_{}.pkl'.format(epoch), 'wb'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(model, open('model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å°æ–°æ¨£æœ¬åšæ¨è«–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# å»ºç«‹æ¸¬è©¦é›†ã€‚é€™é‚Šæˆ‘å€‘å¯ä»¥ç”¨è·Ÿè¨“ç·´æ™‚ä¸åŒçš„ batch_sizeï¼Œçœ‹ä½  GPU å¤šå¤§\n",
    "testset = Dataset(\"test\", tokenizer=tokenizer)\n",
    "testloader = DataLoader(testset, batch_size=4, \n",
    "                        collate_fn=create_mini_batch)\n",
    "\n",
    "# ç”¨åˆ†é¡æ¨¡å‹é æ¸¬æ¸¬è©¦é›†\n",
    "predictions = get_predictions(model, testloader)\n",
    "\n",
    "# ç”¨ä¾†å°‡é æ¸¬çš„ label id è½‰å› label æ–‡å­—\n",
    "index_map = {v: k for k, v in testset.label_map.items()}\n",
    "\n",
    "# ç”Ÿæˆ Kaggle ç¹³äº¤æª”æ¡ˆ\n",
    "df = pd.DataFrame({\"emotion\": predictions.tolist()})\n",
    "df['emotion'] = df.Category.apply(lambda x: index_map[x])\n",
    "df_pred = pd.concat([testset.df.loc[:, [\"ID\"]], \n",
    "                          df.loc[:, 'Category']], axis=1)\n",
    "df_pred.to_csv('bert_submission.csv', index=False)\n",
    "df_pred.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
